{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Final Project\n",
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect and attribute these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "Therefore, this research proposes methods to answer the following 3 research questions [RQ]:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to their source model\n",
    "\n",
    "- **RQ3**. Analysis of the likelihood that different text prompts have to generate authentic images\n",
    "\n",
    "<br>\n",
    "This notebook contains the instructions to test the models that were implemented to anser these questions.<br>\n",
    "In the following sections there are instructions to download the pre-built datasets and the pre-trained weights for the models, in order to test the performance results of this work.<br><br>\n",
    "\n",
    "\n",
    "For a more detailed description of the work done see the complete notebook __[here](Notebook.ipynb)__.<br>\n",
    "The complete notebook can be used to reproduce the entire project from scratch, from the creation of the datasets to the design and training of the models.\n",
    "\n",
    "For furhter informations the complete code of this project can be found in the source directory of the public GitHub repository __[Source Code](https://github.com/parwal-lp/De-Fake_nn_final_project/src)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets\n",
    "First of all it is necessary to download the pre-built and labeled datasets on which the models can be evaluated.<br><br>\n",
    "Download the compressed folder containing all the datasets from this __[link](https://drive.google.com/drive/folders/1Z2qrihz_gKY7R6dula-f0eKjjGxBba6u?usp=sharing)__.<br>\n",
    "Then extract the \"data\" folder and place it at the root of this git repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the pre-traned Weights\n",
    "Then it is also necessary to retrieve the trained weights for all the models to be evaluated.<br><br>\n",
    "They can be downloaded __[here]()__.<br>\n",
    "Then extract the \"trained_models\" folder and place it at the root of this git reporitory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Models\n",
    "\n",
    "For each RQ, the study proposes two possible models to answer the question:\n",
    "\n",
    "1. **Image-only**<br>classifies the image based solely on the input image.\n",
    "\n",
    "2. **Hybrid**<br>classifies the image based on the image together with the text prompt that describes the image and that was used to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries and functions\n",
    "# External libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "# Custom functions implemented for this project\n",
    "from src.imageonly_detector.model import eval_imageonly_detector\n",
    "from src.imageonly_attributor.model import eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, eval_hybrid_detector\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, eval_hybrid_attributor # TODO implement eval function\n",
    "\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom directories for external models\n",
    "clip_dir = \"../../GitHub/CLIP\"\n",
    "proj_dir = \"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only Detector\n",
    "\n",
    "- **Goal:** A classifier that given an image is able to tell if it is real or fake;<br>where fake means that it is generated from a text-to-image generation model.<br><br>\n",
    "- **Model Design:** Binary classifier implemented through a two-layers perceptron.<br><br>\n",
    "- **Dataset:** The model is tested on three different datasets, in which the real images are always fetched from MSCOCO together with their textual descriptions (captions), and fake images are generated respectively by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE models using the captions as prompts.<br>\n",
    "Since the model was trained only on images generated from SD, we expect higher accuracy for that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the dataset...\n",
      "Evaluation starts\n",
      "loading model with trained weights...\n",
      "Evaluation on SD -> Acc: 0.9388 - Loss: 0.2364\n",
      "Evaluation on LD -> Acc: 0.6800 - Loss: 1.4511\n",
      "Evaluation on GLIDE -> Acc: 0.7000 - Loss: 1.1783\n",
      "Evaluation complete in 0m 2s\n"
     ]
    }
   ],
   "source": [
    "# First create Dataloaders\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'val': torchvision.transforms.Compose([ # contains real images from MSCOCO and fake images generated by SD\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_LD': torchvision.transforms.Compose([ # contains real images from MSCOCO and fake images generated by LD\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_GLIDE': torchvision.transforms.Compose([ # contains real images from MSCOCO and fake images generated by GLIDE\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_detector_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['val', 'val_LD', 'val_GLIDE']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['val', 'val_LD', 'val_GLIDE']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['val', 'val_LD', 'val_GLIDE']}\n",
    "\n",
    "# Then evaluate the model on the dataloaders above\n",
    "print(\"Evaluation starts\")\n",
    "print(\"loading model with trained weights...\")\n",
    "imageonly_detector = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "imageonly_detector.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_detector(imageonly_detector, dataloaders, dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid Detector\n",
    "\n",
    "- **Goal:** A classifier that given an image and its textual description is able to tell if it is real or fake.<br>Fake means that it is generated from a text-to-image generation model, and the textual description is the text-prompt used to generate it.<br><br>\n",
    "- **Model Design:** Binary classifier implemented through a two-layers perceptron.<br><br>\n",
    "- **Dataset:** The model is tested on three different datasets (SD, LD and GLIDE), very similar to the image-only attributor case.<br>\n",
    "The difference is that in this case the data samples must contain informations about the images and captions together, so each sample is built as the encoding of an image concatenated with the encoding of its caption (the encodings are generated using the CLIP model).<br>\n",
    "Since the model was trained only on images generated from SD, we expect higher accuracy for that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model with trained weights...\n",
      "Evaluation starts\n",
      "Evaluation on SD --> Accuracy: 0.8933334350585938 - Loss: 0.5949015617370605\n",
      "Evaluation on GLIDE --> Accuracy: 0.7099999189376831 - Loss: 0.6429540514945984\n",
      "Evaluation on LD --> Accuracy: 0.7099999189376831 - Loss: 0.6497436761856079\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained weights on the model\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'], clip_dir, proj_dir)\n",
    "    loss, acc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {acc} - Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Image-only Attributor\n",
    "\n",
    "- **Goal:** A classifier that given an image is able to assign it to its original source (the text-to-image model that generated it).<br><br>\n",
    "- **Model Design:** Multiclass classifier implemented through a two-layer perceptron.<br><br>\n",
    "- **Dataset:** The model is trained and tested on real images and captions fetched from MSCOCO and fake images generated by SD, LD and GLIDE using those captions as prompts (so we have 4 classes in total).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the dataset...\n",
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "Evaluation results -> ACC: 0.7585 - LOSS: 0.9142\n"
     ]
    }
   ],
   "source": [
    "# Build the Dataloaders\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['test']}\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "print(\"loading model with trained weights...\")\n",
    "imageonly_attributor = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "imageonly_attributor.load_state_dict(torch.load('trained_models/imageonly_attributor.pth'))\n",
    "eval_imageonly_attributor(imageonly_attributor, dataloaders, dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hybrid Attributor\n",
    "\n",
    "- **Goal:** A classifier that given an image and its textual description is able to assign it to its original source (the text-to-image model that generated it).<br><br>\n",
    "- **Model Design:** Multiclass classifier implemented through a two-layer perceptron.<br><br>\n",
    "- **Dataset:** The model is tested on datasets very similar to the ones used for the image-only attributor case, so we still have the same 4 classes.<br>\n",
    "The difference is that in this case (as for the hybrid detector model) the data samples must contain informations about the images and captions together, so each sample is built as the encoding of an image concatenated with the encoding of its caption (the encodings are generated using the CLIP model).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Evaluation starts:\n",
      "Evaluation results -> ACC: 0.8649999499320984 - LOSS: 0.5060503482818604\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4)\n",
    "hybrid_attributor.load_state_dict(torch.load('trained_models/hybrid_attributor.pth'))\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/test/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/test\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "dataloader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes, clip_dir, proj_dir)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Evaluation starts:')\n",
    "eval_hybrid_attributor(hybrid_attributor, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
