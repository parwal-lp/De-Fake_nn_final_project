{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Final Project\n",
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect and attribute these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "Therefore, this research proposes methods to answer the following 3 research questions [RQ]:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to their source model\n",
    "\n",
    "- **RQ3**. Analysis of the likelihood that different text prompts have to generate authentic images\n",
    "\n",
    "<br>\n",
    "This notebook contains the instructions to test the models that were implemented to anser these questions.<br>\n",
    "In the following sections there are instructions to download the pre-build datasets and the pre-trained weights for the models, in order to test the performance results of this work.<br><br>\n",
    "\n",
    "\n",
    "For a more detailed description of the work done see the complete notebook __[here](Notebook.ipynb)__.<br>\n",
    "The complete notebook can be used to reproduce the entire project from scratch, from the creation of the datasets to the design and training of the models.\n",
    "\n",
    "For furhter informations the complete code of this project can be found in the source directory of the GitHub repository __[Source Code](https://github.com/parwal-lp/De-Fake_nn_final_project/src)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets\n",
    "Download the dataset from this __[link](https://drive.google.com/drive/folders/1Z2qrihz_gKY7R6dula-f0eKjjGxBba6u?usp=sharing)__.<br>\n",
    "Then extract the \"data\" folder and place it at the root of this git repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the pre-traned Weights\n",
    "Download the trained weights for all the models __[here]()__.<br>\n",
    "Then extract the \"trained_models\" folder and place it at the root of this git reporitory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Models\n",
    "\n",
    "For each RQ, the study proposes two possible models as solution:\n",
    "\n",
    "1. **Image-only**<br>classifies the image based solely on the input image.\n",
    "\n",
    "2. **Hybrid**<br>classifier the image based on the image together with its corresponding text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries and functions\n",
    "# External libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "# Custom functions of this project\n",
    "from src.imageonly_detector.model import eval_imageonly_detector\n",
    "from src.imageonly_attributor.model import eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, eval_hybrid_detector\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, eval_hybrid_attributor # TODO implement eval function\n",
    "\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only Detector\n",
    "\n",
    "Binary classifier implemented through a two-layer perceptron, that is able to tell if an image is real or fake;<br>where fake means that it is generated from a text-to-image generation model.\n",
    "<br><br>\n",
    "The model is tested on real images fetched from MSCOCO and fake images generated by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE.<br>\n",
    "Since the model was trained only on images generated from SD, we expect higher accuracy for that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the dataset...\n",
      "Evaluation starts\n",
      "loading model with trained weights...\n",
      "Evaluation on SD -> Acc: 0.9388 - Loss: 0.2364\n",
      "Evaluation on LD -> Acc: 0.6800 - Loss: 1.4511\n",
      "Evaluation on GLIDE -> Acc: 0.7000 - Loss: 1.1783\n",
      "Evaluation complete in 0m 2s\n"
     ]
    }
   ],
   "source": [
    "# First create Dataloaders\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'val': torchvision.transforms.Compose([ # contains real images from MSCOCO and fake images generated by SD\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_LD': torchvision.transforms.Compose([ # contains real images from MSCOCO and fake images generated by LD\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_GLIDE': torchvision.transforms.Compose([ # contains real images from MSCOCO and fake images generated by GLIDE\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_detector_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['val', 'val_LD', 'val_GLIDE']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['val', 'val_LD', 'val_GLIDE']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['val', 'val_LD', 'val_GLIDE']}\n",
    "\n",
    "# Then evaluate the model on the dataloaders above\n",
    "print(\"Evaluation starts\")\n",
    "print(\"loading model with trained weights...\")\n",
    "imageonly_detector = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "imageonly_detector.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_detector(imageonly_detector, dataloaders, dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid Detector\n",
    "\n",
    "This model is again a binary classifier implemented through a two-layer perceptron and the purpose is the same as in the image-only detector.<br>\n",
    "The difference is that in this case the model will take as input not only the image but also its textual description, which is the text prompt that was used to generate it.\n",
    "<br><br>\n",
    "The model is again tested on real images and captions fetched from MSCOCO and fake images generated by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE.<br>\n",
    "In this case the model was also trained only on images generated from SD, so we again expect higher accuracy for that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model with trained weights...\n",
      "Evaluation starts\n",
      "Evaluation on SD --> Accuracy: 0.8933334350585938 - Loss: 0.5943436622619629\n",
      "Evaluation on GLIDE --> Accuracy: 0.7099999785423279 - Loss: 0.6429539918899536\n",
      "Evaluation on LD --> Accuracy: 0.7099999785423279 - Loss: 0.6497436761856079\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained weights on the model\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'])\n",
    "    loss, acc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {acc} - Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Image-only Attributor\n",
    "Multiclass classifier implemented through a two-layer perceptron, that is able to assign its image to its original source (the text-to-image model that generated it).\n",
    "<br><br>\n",
    "The model is trained and tested on real images fetched from MSCOCO and fake images generated by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (so we have 4 classes in total).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the dataset...\n",
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "Evaluation results -> ACC: 0.3251 - LOSS: 11.5085\n"
     ]
    }
   ],
   "source": [
    "# Build the Dataloaders\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['test']}\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "print(\"loading model with trained weights...\")\n",
    "imageonly_attributor = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "imageonly_attributor.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_attributor(imageonly_attributor, dataloaders, dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hybrid Attributor\n",
    "This model is also a multiclass classifier implemented through a two-layer perceptron, whith the same goal as in the image-only attributor, but in this case the model takes as input not only the image but also its caption (the text prompt used to generate it).\n",
    "<br><br>\n",
    "The model is again trained and tested on real images and captions fetched from MSCOCO and fake images generated by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (so we have 4 classes in total).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Evaluation starts:\n",
      "Evaluation results -> ACC: 0.8649999499320984 - LOSS: 0.4977986812591553\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4)\n",
    "hybrid_attributor.load_state_dict(torch.load('trained_models/hybrid_attributor.pth'))\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/test/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/test\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "dataloader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Evaluation starts:')\n",
    "eval_hybrid_attributor(hybrid_attributor, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
