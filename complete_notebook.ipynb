{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect and attribute these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "This project proposes methods to answer 2 of the research questions [RQ] proposed in the paper:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models (See sections 1 and 2)\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to the text-to-image model that generated it (See sections 3 and 4)\n",
    "\n",
    "<br>\n",
    "This notebook contains the instructions to reproduce the entire project from scratch, from the creation of the datasets to the design and training of the models.<br><br>\n",
    "\n",
    "For quick examples on how to use the implemented models see the __[tldrNotebook](tldr_notebook.ipynb)__, where the pre-built datasets and pre-trained weights can be downloaded.<br>\n",
    "\n",
    "For furhter informations the complete code of this project can be found in the source directory of the public GitHub repository __[Source Code](https://github.com/parwal-lp/De-Fake_nn_final_project/src)__.\n",
    "\n",
    "Before proceeding to run the code in this Notebook, please read the instructions contained in the __[Readme](https://github.com/parwal-lp/De-Fake_nn_final_project/tree/main)__ on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reproduce this project from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the path variables to be used globally in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these paths as described below\n",
    "proj_dir = \"/home/parwal/Documents/test/De-Fake_nn_final_project\"    # set here the absolute path to the root of the current project (De-Fake)\n",
    "clip_dir = \"/home/parwal/Documents/GitHub/CLIP\"    # set here the absolute path to the CLIP directory cloned from GitHub\n",
    "ld_dir = \"/home/parwal/Documents/GitHub/latent-diffusion\"   # set here the absolute path to the LD directory cloned from GitHub\n",
    "glide_dir = \"/home/parwal/Documents/GitHub/glide-text2im\"   # set here the absolute path to the GLIDE directory cloned from GitHub\n",
    "\n",
    "SD_api_key = 'sk-6MTDQWuQSLiU3SIc8GEkQrFK7Yjh85JIj0nTfZZKFwircCQQ' # Set here your Stable Diffusion API key (with enough credit to generate at least 400 images\n",
    "\n",
    "# Do not change these paths, they are part of the implementation\n",
    "SD_generated_temp_dir = \"data/generated/SD+MSCOCO/\"\n",
    "GLIDE_generated_temp_dir = \"data/generated/GLIDE+MSCOCO/\"\n",
    "LD_generated_temp_dir = ld_dir + \"/outputs/txt2img-samples/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare all the imports needed in this notebook\n",
    "\n",
    "# External libraries imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# References to other files of this project\n",
    "# Functions for the management of data\n",
    "from src.data_collector import fetchImagesFromMSCOCO\n",
    "from src.dataset_generator import SD_generation, LD_generation, GLIDE_generation\n",
    "from src.format_dataset import format_dataset_binaryclass, formatIntoTrainTest, format_dataset_multiclass\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader\n",
    "# Functions for building and training the models\n",
    "from src.imageonly_detector.model import train_imageonly_detector, eval_imageonly_detector\n",
    "from src.imageonly_attributor.model import train_imageonly_attributor, eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, train_hybrid_detector, eval_hybrid_detector\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, train_hybrid_attributor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1. Detection of images generated by text-to-image generation models\n",
    "\n",
    "The study proposes two detector models:\n",
    "\n",
    "1. **Image-only detector**<br>binary classifier that decides whether an input image is fake or real.\n",
    "\n",
    "2. **Hybrid detector**<br>binary classifier that is able to tell if an image is fake or real, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only detector\n",
    "This model is implemented as a two-layer perceptron, to be used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "All the datasets are constitueted by a set of N real images (labeled 1), and a set of N corresponding fake generated images (labeled 0).\n",
    "\n",
    "Training (on a single dataset):\n",
    "- real images fetched from MSCOCO (class 1)\n",
    "- fake images generated by Stable Diffusion (SD) (class 0)\n",
    "\n",
    "Evaluation (on 3 different datasets):\n",
    "- real images always fetched from MSCOCO (class 1)\n",
    "- fake images generated respectively by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (class 0)\n",
    "\n",
    "The data is structured as follows:\n",
    "\n",
    "imageonly_detector_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by LD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fetch the real images together with their captions, for all the datasets described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO\n",
    "# The dataset generated using SD will be divided into train and test later on, it is temporarely saved in the directory fetched/MSCOCO_for_SD\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/fetched/MSCOCO_for_SD\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# real images and captions needed as input for the LD model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_LD/class_1\", \"data/imageonly_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "# real images and captions needed as input for the GLIDE model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_GLIDE/class_1\", \"data/imageonly_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously fetched captions to generate the fake images.<br><br>\n",
    "Notice that for SD we use the APIs, while for LD and GLIDE we used a downloaded local model.<br><br>\n",
    "Also, SD has a very strict protection against inappropriate text prompts, so it might refuse to process some of the prompts, even if they are legit.<br>\n",
    "This exception is handled by the implemented methods, and ignores the unprocessed images labeling them as invalid, those won't be part of the datasets, and even the real counterparts are not included in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/fetched/MSCOCO_for_SD/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "#use Latent Diffusion model to generate 50 images starting from the captions fetched before\n",
    "print(\"generating images using LD...\")\n",
    "LD_generation(\"data/imageonly_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "print(\"LD images generated successfully!\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#use GLIDE model to generate 50 images starting from the captions fetched before\n",
    "print(\"generating images using GLIDE...\")\n",
    "GLIDE_generation(\"data/imageonly_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "print(\"GLIDE images generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the collected and generated images from their respective folders into the dataset directory structured as described before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/fetched/MSCOCO_for_SD\", SD_generated_temp_dir, \"data/imageonly_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/imageonly_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/imageonly_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 1.1 (jpg images)<br>Each item of this dataset is transformed in order to achieve better performance.<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader obtained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 3.3088 Acc: 0.5000\n",
      "val Loss: 0.9897 Acc: 0.6837\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.9256 Acc: 0.6900\n",
      "val Loss: 1.2000 Acc: 0.6633\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.4523 Acc: 0.8300\n",
      "val Loss: 0.7692 Acc: 0.7551\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.3544 Acc: 0.8500\n",
      "val Loss: 0.3384 Acc: 0.8673\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.4832 Acc: 0.8300\n",
      "val Loss: 0.5524 Acc: 0.8163\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.4608 Acc: 0.8600\n",
      "val Loss: 0.5301 Acc: 0.8367\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.2652 Acc: 0.9000\n",
      "val Loss: 0.1683 Acc: 0.9388\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.3290 Acc: 0.9100\n",
      "val Loss: 0.2725 Acc: 0.9184\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9400\n",
      "val Loss: 0.1601 Acc: 0.9388\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.1026 Acc: 0.9700\n",
      "val Loss: 0.3649 Acc: 0.9082\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.1676 Acc: 0.9300\n",
      "val Loss: 0.1860 Acc: 0.9388\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9400\n",
      "val Loss: 0.2030 Acc: 0.9184\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.6131 Acc: 0.7800\n",
      "val Loss: 0.2018 Acc: 0.8980\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.2035 Acc: 0.9400\n",
      "val Loss: 0.1541 Acc: 0.9592\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.1688 Acc: 0.9300\n",
      "val Loss: 0.2177 Acc: 0.9082\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.3727 Acc: 0.8600\n",
      "val Loss: 0.1729 Acc: 0.9184\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.4113 Acc: 0.8500\n",
      "val Loss: 0.3785 Acc: 0.9082\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.3501 Acc: 0.8900\n",
      "val Loss: 0.2380 Acc: 0.9184\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.2245 Acc: 0.9200\n",
      "val Loss: 0.2422 Acc: 0.8980\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.2736 Acc: 0.9000\n",
      "val Loss: 0.2355 Acc: 0.9184\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.2466 Acc: 0.9000\n",
      "val Loss: 0.1564 Acc: 0.9286\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.9200\n",
      "val Loss: 0.2745 Acc: 0.8980\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.3206 Acc: 0.8800\n",
      "val Loss: 0.1553 Acc: 0.9388\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.3000 Acc: 0.9200\n",
      "val Loss: 0.2505 Acc: 0.9184\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.1850 Acc: 0.9200\n",
      "val Loss: 0.2153 Acc: 0.8980\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.2282 Acc: 0.9100\n",
      "val Loss: 0.2281 Acc: 0.8878\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.2167 Acc: 0.9500\n",
      "val Loss: 0.1826 Acc: 0.9388\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.2759 Acc: 0.9300\n",
      "val Loss: 0.2291 Acc: 0.9286\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0580 Acc: 0.9900\n",
      "val Loss: 0.3235 Acc: 0.8776\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.3211 Acc: 0.8800\n",
      "val Loss: 0.1944 Acc: 0.9184\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.2959 Acc: 0.8800\n",
      "val Loss: 0.2929 Acc: 0.8776\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3730 Acc: 0.8900\n",
      "val Loss: 0.3030 Acc: 0.8673\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.2847 Acc: 0.8900\n",
      "val Loss: 0.2215 Acc: 0.9184\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.1547 Acc: 0.9500\n",
      "val Loss: 0.1918 Acc: 0.9388\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.2869 Acc: 0.8400\n",
      "val Loss: 0.2328 Acc: 0.9082\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.2429 Acc: 0.9000\n",
      "val Loss: 0.1789 Acc: 0.9184\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.3787 Acc: 0.8500\n",
      "val Loss: 0.2128 Acc: 0.8878\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.2615 Acc: 0.9200\n",
      "val Loss: 0.2607 Acc: 0.8776\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.2128 Acc: 0.9200\n",
      "val Loss: 0.2323 Acc: 0.9082\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2688 Acc: 0.9200\n",
      "val Loss: 0.2833 Acc: 0.8673\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.1151 Acc: 0.9500\n",
      "val Loss: 0.3655 Acc: 0.8980\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.2133 Acc: 0.9300\n",
      "val Loss: 0.2756 Acc: 0.8878\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.3816 Acc: 0.8600\n",
      "val Loss: 0.3421 Acc: 0.8878\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.2626 Acc: 0.9300\n",
      "val Loss: 0.2628 Acc: 0.8980\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.2292 Acc: 0.9000\n",
      "val Loss: 0.1617 Acc: 0.9388\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.1650 Acc: 0.9300\n",
      "val Loss: 0.3155 Acc: 0.8980\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.1057 Acc: 0.9600\n",
      "val Loss: 0.1654 Acc: 0.9082\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.1878 Acc: 0.9300\n",
      "val Loss: 0.3213 Acc: 0.8673\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.2458 Acc: 0.9200\n",
      "val Loss: 0.2180 Acc: 0.8878\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.3046 Acc: 0.8700\n",
      "val Loss: 0.1369 Acc: 0.9388\n",
      "\n",
      "Training complete in 0m 59s\n",
      "Best val Acc: 0.959184\n",
      "Evaluation starts\n",
      "loading model with trained weights...\n",
      "Evaluation on SD -> Acc: 0.9184 - Loss: 0.1917\n",
      "Evaluation on LD -> Acc: 0.7900 - Loss: 0.6790\n",
      "Evaluation on GLIDE -> Acc: 0.7600 - Loss: 0.6742\n",
      "Evaluation complete in 0m 1s\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = 'data/imageonly_detector_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts\")\n",
    "trained_model = train_imageonly_detector(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "# Load a model with the trained weights from the previous step, and evaluate it on test data\n",
    "print(\"Evaluation starts\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_detector(test_model, dataloaders, dataset_sizes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid detector\n",
    "For this problem we again implement a two-layer perceptron for binary classification, but in this case it will take as input not only the images but also their captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "\n",
    "The data is first fetched and generated in the exact same way as the dataset for the image-only detector.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- COLLECT REAL IMAGES FROM MSCOCO -------------------- #\n",
    "\n",
    "#SD+MSCOCO\n",
    "print(\"fetching images for SD...\")\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/hybrid_detector_data\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"fetching images for LD...\")\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_LD/class_1\", \"data/hybrid_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"fetching images for GLIDE...\")\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_GLIDE/class_1\", \"data/hybrid_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- GENERATE FAKE IMAGES USING SD, LD, GLIDE -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/hybrid_detector_data/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"generating images using LD...\")\n",
    "LD_generation(\"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "print(\"LD images generated successfully!\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"generating images using GLIDE...\")\n",
    "GLIDE_generation(\"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "print(\"GLIDE images generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FORMAT THE DATA INTO THE STRUCTURE NEEDED FOR TRAINING/TESTING -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "print(\"Building SD dataset...\")\n",
    "formatIntoTrainTest(\"data/fetched/MSCOCO_for_SD/\", SD_generated_temp_dir, \"data/hybrid_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"Building LD dataset...\")\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/hybrid_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"Building GLIDE dataset...\")\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/hybrid_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module.<br>A two-layer perceptron that outputs 0 (fake) or 1 (real) for each sample.<br><br>\n",
    "- **Create Dataset and DataLoader** starting from the row data fetched at 2.1 (jpg images and string captions).<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model.<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "print('Building the model...')\n",
    "hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "\n",
    "#Build the dataset\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_detector_data/mscoco_captions.csv\"\n",
    "real_img_dir = \"data/hybrid_detector_data/train/class_1\"\n",
    "fake_img_dir = \"data/hybrid_detector_data/train/class_0\"\n",
    "train_data_loader = get_dataset_loader(captions_file, real_img_dir, fake_img_dir, clip_dir, proj_dir)\n",
    "\n",
    "#Train the model\n",
    "print('Training starts')\n",
    "train_hybrid_detector(hybrid_detector, train_data_loader, 40, 0.0009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it on some test datasets.<br>\n",
    "In particular we will evaluate it on:<br>\n",
    "- Stable Diffusion (SD), dataset generated from the same image-to-text generator used for the train dataset.\n",
    "- GLIDE\n",
    "- Latent Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the weights we trained in the previous code block\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'], clip_dir, proj_dir)\n",
    "    SDloss, SDacc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {SDacc} - Loss: {SDloss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2. Attribution of the fake images to their source model\n",
    "\n",
    "The study proposes two attributor models:\n",
    "\n",
    "1. **Image-only attributor**<br>multi-class classifier that assigns each input image to its source generation model, given the image only.\n",
    "\n",
    "2. **Hybrid attributor**<br>multi-class classifier that assigns each input image to its source generation model, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only attributor\n",
    "\n",
    "In this section we will build and train a model that is able to assign an image to the model that generated it, given only that image.<br><br>\n",
    "The classes that this model will be able to address are the following:\n",
    "- real image -> class 0\n",
    "- fake image generated by SD -> class 1\n",
    "- fake image generated by LD -> class 2\n",
    "- fake image generated by GLIDE -> class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "\n",
    "We generate two datasets, one for training and one for evaluating the model.<br>\n",
    "The steps needed to generate the two datasets are the same:\n",
    "- fetch real images and their captions from MSCOCO (class 0)\n",
    "- generate fake images with SD using the captions of the real images (class 1)\n",
    "- generate fake images with LD using the captions of the real images (class 2)\n",
    "- generate fake images with GLIDE using the captions of the real images (class 3)\n",
    "- move the real and generated images into a dataset directory, with the following structure:\n",
    "\n",
    "imageonly_attributor_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_real/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_SD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by GLIDE*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by LD*<br>\n",
    "&emsp;&emsp;├── test/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sposto le immagini LD\n",
      "sposto le immagini SD\n",
      "['84610.jpg', '331551.jpg', '434986.jpg', '89589.jpg', '165835.jpg', '264884.jpg', '471335.jpg', '391519.jpg', '117466.jpg', '29715.jpg', '564204.jpg', '27157.jpg']\n",
      "sposto le immagini GLIDE\n"
     ]
    }
   ],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/train/class_real\", \"data/imageonly_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sposto le immagini LD\n",
      "sposto le immagini SD\n",
      "['84610.jpg', '331551.jpg', '434986.jpg', '89589.jpg', '516774.jpg', '165835.jpg', '264884.jpg', '471335.jpg', '391519.jpg', '117466.jpg', '29715.jpg', '564204.jpg', '451012.jpg', '27157.jpg']\n",
      "sposto le immagini GLIDE\n"
     ]
    }
   ],
   "source": [
    "# Repeat the same procedure for the test dataset\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/test/class_real\", \"data/imageonly_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 1.1 (jpg images)<br>Each item of this dataset is transformed in order to obtain better generalization<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 6.4730 Acc: 0.1357\n",
      "test Loss: 4.1937 Acc: 0.3283\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.1459 Acc: 0.6633\n",
      "test Loss: 2.2066 Acc: 0.5404\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.3689 Acc: 0.7487\n",
      "test Loss: 1.1153 Acc: 0.7323\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.3959 Acc: 0.6985\n",
      "test Loss: 1.8747 Acc: 0.6162\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.9474 Acc: 0.7487\n",
      "test Loss: 1.2790 Acc: 0.7323\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.7944 Acc: 0.8141\n",
      "test Loss: 1.3391 Acc: 0.6212\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.5875 Acc: 0.8191\n",
      "test Loss: 1.1329 Acc: 0.6414\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.5855 Acc: 0.8442\n",
      "test Loss: 1.0106 Acc: 0.7222\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.4750 Acc: 0.8693\n",
      "test Loss: 1.2423 Acc: 0.6313\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.3602 Acc: 0.8643\n",
      "test Loss: 1.2142 Acc: 0.6919\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.5698 Acc: 0.7990\n",
      "test Loss: 1.4487 Acc: 0.6818\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.5413 Acc: 0.8442\n",
      "test Loss: 1.0049 Acc: 0.6818\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.3958 Acc: 0.8844\n",
      "test Loss: 1.0912 Acc: 0.6970\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.5242 Acc: 0.8392\n",
      "test Loss: 1.0323 Acc: 0.6566\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.4630 Acc: 0.8442\n",
      "test Loss: 0.9918 Acc: 0.6869\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.4088 Acc: 0.8794\n",
      "test Loss: 1.0560 Acc: 0.6818\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.3422 Acc: 0.8794\n",
      "test Loss: 0.8875 Acc: 0.7677\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.5104 Acc: 0.8392\n",
      "test Loss: 0.7247 Acc: 0.7778\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.3926 Acc: 0.8342\n",
      "test Loss: 0.9933 Acc: 0.6869\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.4035 Acc: 0.8744\n",
      "test Loss: 1.0528 Acc: 0.7374\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.3550 Acc: 0.8894\n",
      "test Loss: 1.0625 Acc: 0.6869\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.3789 Acc: 0.8693\n",
      "test Loss: 1.6836 Acc: 0.6212\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4128 Acc: 0.8894\n",
      "test Loss: 0.8019 Acc: 0.7222\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.2805 Acc: 0.8894\n",
      "test Loss: 0.9073 Acc: 0.7121\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.1783 Acc: 0.9296\n",
      "test Loss: 0.6806 Acc: 0.7879\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.9598\n",
      "test Loss: 0.9834 Acc: 0.7121\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.2174 Acc: 0.9196\n",
      "test Loss: 1.0556 Acc: 0.7121\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.3886 Acc: 0.8693\n",
      "test Loss: 0.7149 Acc: 0.7576\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.2403 Acc: 0.9196\n",
      "test Loss: 1.5844 Acc: 0.6162\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.4080 Acc: 0.8744\n",
      "test Loss: 1.1482 Acc: 0.6919\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.5848 Acc: 0.8543\n",
      "test Loss: 1.0644 Acc: 0.6970\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3186 Acc: 0.8995\n",
      "test Loss: 0.8004 Acc: 0.6919\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.3454 Acc: 0.8744\n",
      "test Loss: 0.7619 Acc: 0.7424\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.3564 Acc: 0.8894\n",
      "test Loss: 0.6990 Acc: 0.7374\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.2462 Acc: 0.9095\n",
      "test Loss: 0.8890 Acc: 0.7121\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 0.9296\n",
      "test Loss: 0.9353 Acc: 0.7020\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.3425 Acc: 0.8995\n",
      "test Loss: 0.6342 Acc: 0.7828\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.3496 Acc: 0.8794\n",
      "test Loss: 0.9680 Acc: 0.7121\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.4418 Acc: 0.8593\n",
      "test Loss: 0.6472 Acc: 0.7980\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2074 Acc: 0.9296\n",
      "test Loss: 0.8659 Acc: 0.7626\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.2653 Acc: 0.9196\n",
      "test Loss: 0.8683 Acc: 0.7121\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.2920 Acc: 0.9095\n",
      "test Loss: 1.1682 Acc: 0.6616\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.2058 Acc: 0.9196\n",
      "test Loss: 0.7168 Acc: 0.7778\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.2879 Acc: 0.8794\n",
      "test Loss: 0.8984 Acc: 0.7475\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.1691 Acc: 0.9397\n",
      "test Loss: 0.8766 Acc: 0.7374\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.1523 Acc: 0.9497\n",
      "test Loss: 0.9282 Acc: 0.7374\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.2119 Acc: 0.9196\n",
      "test Loss: 0.7668 Acc: 0.7525\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.9095\n",
      "test Loss: 0.8604 Acc: 0.7222\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.2739 Acc: 0.8945\n",
      "test Loss: 1.1760 Acc: 0.6465\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.2559 Acc: 0.9447\n",
      "test Loss: 0.9599 Acc: 0.7374\n",
      "Best val Acc: 0.797980\n",
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "Evaluation results -> ACC: 0.7980 - LOSS: 0.6471\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts:\")\n",
    "trained_model = train_imageonly_attributor(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_attributor.pth'))\n",
    "eval_imageonly_attributor(test_model, dataloaders, dataset_sizes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid attributor\n",
    "In this section we will build and train a model similar to the model built in section 1.<br>\n",
    "The difference is that instead of taking as input only the image, this model also considers its textual caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test datasets are generated in the same way as in the image-only attributor case.<br>\n",
    "For the dataset directory structure also refer to the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sposto le immagini LD\n",
      "sposto le immagini SD\n",
      "['84610.jpg', '331551.jpg', '333946.jpg', '434986.jpg', '89589.jpg', '516774.jpg', '165835.jpg', '264884.jpg', '471335.jpg', '391519.jpg', '117466.jpg', '29715.jpg', '564204.jpg', '451012.jpg', '27157.jpg']\n",
      "sposto le immagini GLIDE\n"
     ]
    }
   ],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/train/class_real\", \"data/hybrid_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then denerate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/test/class_real\", \"data/hybrid_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs the class predicted for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "EPOCH:  1/30  - MEAN ACCURACY:  tensor(0.2333)  - MEAN LOSS:  tensor(1.3932)\n",
      "EPOCH:  2/30  - MEAN ACCURACY:  tensor(0.3850)  - MEAN LOSS:  tensor(1.3695)\n",
      "EPOCH:  3/30  - MEAN ACCURACY:  tensor(0.4267)  - MEAN LOSS:  tensor(1.3462)\n",
      "EPOCH:  4/30  - MEAN ACCURACY:  tensor(0.5567)  - MEAN LOSS:  tensor(1.3228)\n",
      "EPOCH:  5/30  - MEAN ACCURACY:  tensor(0.6167)  - MEAN LOSS:  tensor(1.2970)\n",
      "EPOCH:  6/30  - MEAN ACCURACY:  tensor(0.6600)  - MEAN LOSS:  tensor(1.2687)\n",
      "EPOCH:  7/30  - MEAN ACCURACY:  tensor(0.6867)  - MEAN LOSS:  tensor(1.2389)\n",
      "EPOCH:  8/30  - MEAN ACCURACY:  tensor(0.7700)  - MEAN LOSS:  tensor(1.2037)\n",
      "EPOCH:  9/30  - MEAN ACCURACY:  tensor(0.8250)  - MEAN LOSS:  tensor(1.1633)\n",
      "EPOCH:  10/30  - MEAN ACCURACY:  tensor(0.7767)  - MEAN LOSS:  tensor(1.1255)\n",
      "EPOCH:  11/30  - MEAN ACCURACY:  tensor(0.8367)  - MEAN LOSS:  tensor(1.0807)\n",
      "EPOCH:  12/30  - MEAN ACCURACY:  tensor(0.8467)  - MEAN LOSS:  tensor(1.0347)\n",
      "EPOCH:  13/30  - MEAN ACCURACY:  tensor(0.8800)  - MEAN LOSS:  tensor(0.9828)\n",
      "EPOCH:  14/30  - MEAN ACCURACY:  tensor(0.8950)  - MEAN LOSS:  tensor(0.9311)\n",
      "EPOCH:  15/30  - MEAN ACCURACY:  tensor(0.8950)  - MEAN LOSS:  tensor(0.8807)\n",
      "EPOCH:  16/30  - MEAN ACCURACY:  tensor(0.8950)  - MEAN LOSS:  tensor(0.8270)\n",
      "EPOCH:  17/30  - MEAN ACCURACY:  tensor(0.9050)  - MEAN LOSS:  tensor(0.7771)\n",
      "EPOCH:  18/30  - MEAN ACCURACY:  tensor(0.9300)  - MEAN LOSS:  tensor(0.7269)\n",
      "EPOCH:  19/30  - MEAN ACCURACY:  tensor(0.9250)  - MEAN LOSS:  tensor(0.6840)\n",
      "EPOCH:  20/30  - MEAN ACCURACY:  tensor(0.9417)  - MEAN LOSS:  tensor(0.6381)\n",
      "EPOCH:  21/30  - MEAN ACCURACY:  tensor(0.9350)  - MEAN LOSS:  tensor(0.5849)\n",
      "EPOCH:  22/30  - MEAN ACCURACY:  tensor(0.9500)  - MEAN LOSS:  tensor(0.5530)\n",
      "EPOCH:  23/30  - MEAN ACCURACY:  tensor(0.9450)  - MEAN LOSS:  tensor(0.5168)\n",
      "EPOCH:  24/30  - MEAN ACCURACY:  tensor(0.9500)  - MEAN LOSS:  tensor(0.4793)\n",
      "EPOCH:  25/30  - MEAN ACCURACY:  tensor(0.9550)  - MEAN LOSS:  tensor(0.4507)\n",
      "EPOCH:  26/30  - MEAN ACCURACY:  tensor(0.9700)  - MEAN LOSS:  tensor(0.4163)\n",
      "EPOCH:  27/30  - MEAN ACCURACY:  tensor(0.9600)  - MEAN LOSS:  tensor(0.3921)\n",
      "EPOCH:  28/30  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3672)\n",
      "EPOCH:  29/30  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3417)\n",
      "EPOCH:  30/30  - MEAN ACCURACY:  tensor(0.9850)  - MEAN LOSS:  tensor(0.3188)\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4)\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/train/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/train\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "train_data_loader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes, clip_dir, proj_dir)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Training starts:')\n",
    "train_hybrid_attributor(hybrid_attributor, train_data_loader, 30, 0.005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3. Analysis of the likelihood that different text prompts have to generate authentic images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
