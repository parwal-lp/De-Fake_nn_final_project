{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect and attribute these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "This project proposes methods to answer 2 of the research questions [RQ] proposed in the paper:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models (See sections 1 and 2)\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to the text-to-image model that generated it (See sections 3 and 4)\n",
    "\n",
    "<br>\n",
    "This notebook contains the instructions to reproduce the entire project from scratch, from the creation of the datasets to the design and training of the models.<br><br>\n",
    "\n",
    "For quick examples on how to use the implemented models see the __[tldrNotebook](tldr_notebook.ipynb)__, where the pre-built datasets and pre-trained weights can be downloaded.<br>\n",
    "\n",
    "For furhter informations the complete code of this project can be found in the source directory of the public GitHub repository __[Source Code](https://github.com/parwal-lp/De-Fake_nn_final_project/tree/main/src)__.\n",
    "\n",
    "Before proceeding to run the code in this Notebook, please read the instructions contained in the __[Readme](https://github.com/parwal-lp/De-Fake_nn_final_project/tree/main)__ on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reproduce this project from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the path variables to be used globally in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these paths as described below\n",
    "proj_dir = \"/home/parwal/Documents/test/De-Fake_nn_final_project\"    # set here the absolute path to the root of the current project (De-Fake)\n",
    "clip_dir = \"/home/parwal/Documents/GitHub/CLIP\"    # set here the absolute path to the CLIP directory cloned from GitHub\n",
    "ld_dir = \"/home/parwal/Documents/GitHub/latent-diffusion\"   # set here the absolute path to the LD directory cloned from GitHub\n",
    "glide_dir = \"/home/parwal/Documents/GitHub/glide-text2im\"   # set here the absolute path to the GLIDE directory cloned from GitHub\n",
    "\n",
    "SD_api_key = 'sk-6MTDQWuQSLiU3SIc8GEkQrFK7Yjh85JIj0nTfZZKFwircCQQ' # Set here your Stable Diffusion API key (with enough credit to generate at least 400 images\n",
    "\n",
    "# Do not change these paths, they are part of the implementation\n",
    "SD_generated_temp_dir = \"data/generated/SD+MSCOCO/\"\n",
    "GLIDE_generated_temp_dir = \"data/generated/GLIDE+MSCOCO/\"\n",
    "LD_generated_temp_dir = ld_dir + \"/outputs/txt2img-samples/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare all the imports needed in this notebook\n",
    "\n",
    "# External libraries imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# References to other files of this project\n",
    "# Functions for the management of data\n",
    "from src.data_collector import fetchImagesFromMSCOCO\n",
    "from src.dataset_generator import SD_generation, LD_generation, GLIDE_generation\n",
    "from src.format_dataset import format_dataset_binaryclass, formatIntoTrainTest, format_dataset_multiclass\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader\n",
    "# Functions for building and training the models\n",
    "from src.imageonly_detector.model import train_imageonly_detector, eval_imageonly_detector\n",
    "from src.imageonly_attributor.model import train_imageonly_attributor, eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, train_hybrid_detector, eval_hybrid_detector\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, train_hybrid_attributor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1. Detection of images generated by text-to-image generation models\n",
    "\n",
    "The study proposes two detector models:\n",
    "\n",
    "1. **Image-only detector**<br>binary classifier that decides whether an input image is fake or real.\n",
    "\n",
    "2. **Hybrid detector**<br>binary classifier that is able to tell if an image is fake or real, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only detector\n",
    "This model is implemented as a two-layer perceptron, to be used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "All the datasets are constitueted by a set of N real images (labeled 1), and a set of N corresponding fake generated images (labeled 0).\n",
    "\n",
    "Training (on a single dataset):\n",
    "- real images fetched from MSCOCO (class 1)\n",
    "- fake images generated by Stable Diffusion (SD) (class 0)\n",
    "\n",
    "Evaluation (on 3 different datasets):\n",
    "- real images always fetched from MSCOCO (class 1)\n",
    "- fake images generated respectively by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (class 0)\n",
    "\n",
    "The data is structured as follows:\n",
    "\n",
    "imageonly_detector_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by LD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fetch the real images together with their captions, for all the datasets described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO\n",
    "# The dataset generated using SD will be divided into train and test later on, it is temporarely saved in the directory fetched/MSCOCO_for_SD\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/fetched/MSCOCO_for_SD\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# real images and captions needed as input for the LD model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_LD/class_1\", \"data/imageonly_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "# real images and captions needed as input for the GLIDE model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_GLIDE/class_1\", \"data/imageonly_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously fetched captions to generate the fake images.<br><br>\n",
    "Notice that for SD we use the APIs, while for LD and GLIDE we used a downloaded local model.<br><br>\n",
    "Also, SD has a very strict protection against inappropriate text prompts, so it might refuse to process some of the prompts, even if they are legit.<br>\n",
    "This exception is handled by the implemented methods, and ignores the unprocessed images labeling them as invalid, those won't be part of the datasets, and even the real counterparts are not included in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/fetched/MSCOCO_for_SD/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "#use Latent Diffusion model to generate 50 images starting from the captions fetched before\n",
    "print(\"generating images using LD...\")\n",
    "LD_generation(\"data/imageonly_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "print(\"LD images generated successfully!\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#use GLIDE model to generate 50 images starting from the captions fetched before\n",
    "print(\"generating images using GLIDE...\")\n",
    "GLIDE_generation(\"data/imageonly_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "print(\"GLIDE images generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the collected and generated images from their respective folders into the dataset directory structured as described before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/fetched/MSCOCO_for_SD\", SD_generated_temp_dir, \"data/imageonly_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/imageonly_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/imageonly_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 1.1 (jpg images)<br>Each item of this dataset is transformed in order to achieve better performance.<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader obtained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = 'data/imageonly_detector_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts\")\n",
    "trained_model = train_imageonly_detector(model, dataloaders, dataset_sizes, num_epochs=20)\n",
    "\n",
    "# Load a model with the trained weights from the previous step, and evaluate it on test data\n",
    "print(\"Evaluation starts\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_detector(test_model, dataloaders, dataset_sizes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid detector\n",
    "For this problem we again implement a two-layer perceptron for binary classification, but in this case it will take as input not only the images but also their captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "\n",
    "The data is first fetched and generated in the exact same way as the dataset for the image-only detector.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- COLLECT REAL IMAGES FROM MSCOCO -------------------- #\n",
    "\n",
    "#SD+MSCOCO\n",
    "print(\"fetching images for SD...\")\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/hybrid_detector_data\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"fetching images for LD...\")\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_LD/class_1\", \"data/hybrid_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"fetching images for GLIDE...\")\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_GLIDE/class_1\", \"data/hybrid_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- GENERATE FAKE IMAGES USING SD, LD, GLIDE -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/hybrid_detector_data/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"generating images using LD...\")\n",
    "LD_generation(\"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "print(\"LD images generated successfully!\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"generating images using GLIDE...\")\n",
    "GLIDE_generation(\"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "print(\"GLIDE images generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FORMAT THE DATA INTO THE STRUCTURE NEEDED FOR TRAINING/TESTING -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "print(\"Building SD dataset...\")\n",
    "formatIntoTrainTest(\"data/fetched/MSCOCO_for_SD/\", SD_generated_temp_dir, \"data/hybrid_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"Building LD dataset...\")\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/hybrid_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"Building GLIDE dataset...\")\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/hybrid_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module.<br>A two-layer perceptron that outputs 0 (fake) or 1 (real) for each sample.<br><br>\n",
    "- **Create Dataset and DataLoader** starting from the row data fetched at 2.1 (jpg images and string captions).<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model.<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "print('Building the model...')\n",
    "hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "\n",
    "#Build the dataset\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_detector_data/mscoco_captions.csv\"\n",
    "real_img_dir = \"data/hybrid_detector_data/train/class_1\"\n",
    "fake_img_dir = \"data/hybrid_detector_data/train/class_0\"\n",
    "train_data_loader = get_dataset_loader(captions_file, real_img_dir, fake_img_dir, clip_dir, proj_dir)\n",
    "\n",
    "#Train the model\n",
    "print('Training starts')\n",
    "train_hybrid_detector(hybrid_detector, train_data_loader, 40, 0.0009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it on some test datasets.<br>\n",
    "In particular we will evaluate it on:<br>\n",
    "- Stable Diffusion (SD), dataset generated from the same image-to-text generator used for the train dataset.\n",
    "- GLIDE\n",
    "- Latent Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the weights we trained in the previous code block\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'], clip_dir, proj_dir)\n",
    "    SDloss, SDacc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {SDacc} - Loss: {SDloss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2. Attribution of the fake images to their source model\n",
    "\n",
    "The study proposes two attributor models:\n",
    "\n",
    "1. **Image-only attributor**<br>multi-class classifier that assigns each input image to its source generation model, given the image only.\n",
    "\n",
    "2. **Hybrid attributor**<br>multi-class classifier that assigns each input image to its source generation model, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only attributor\n",
    "\n",
    "In this section we will build and train a model that is able to assign an image to the model that generated it, given only that image.<br><br>\n",
    "The classes that this model will be able to address are the following:\n",
    "- real image -> class 0\n",
    "- fake image generated by SD -> class 1\n",
    "- fake image generated by LD -> class 2\n",
    "- fake image generated by GLIDE -> class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "\n",
    "We generate two datasets, one for training and one for evaluating the model.<br>\n",
    "The steps needed to generate the two datasets are the same:\n",
    "- fetch real images and their captions from MSCOCO (class 0)\n",
    "- generate fake images with SD using the captions of the real images (class 1)\n",
    "- generate fake images with LD using the captions of the real images (class 2)\n",
    "- generate fake images with GLIDE using the captions of the real images (class 3)\n",
    "- move the real and generated images into a dataset directory, with the following structure:\n",
    "\n",
    "imageonly_attributor_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_real/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_SD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by GLIDE*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by LD*<br>\n",
    "&emsp;&emsp;├── test/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/train/class_real\", \"data/imageonly_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same procedure for the test dataset\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/test/class_real\", \"data/imageonly_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 1.1 (jpg images)<br>Each item of this dataset is transformed in order to obtain better generalization<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts:\")\n",
    "trained_model = train_imageonly_attributor(model, dataloaders, dataset_sizes, num_epochs=30)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_attributor.pth'))\n",
    "eval_imageonly_attributor(test_model, dataloaders, dataset_sizes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid attributor\n",
    "In this section we will build and train a model similar to the model built in section 1.<br>\n",
    "The difference is that instead of taking as input only the image, this model also considers its textual caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test datasets are generated in the same way as in the image-only attributor case.<br>\n",
    "For the dataset directory structure also refer to the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/train/class_real\", \"data/hybrid_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then denerate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/test/class_real\", \"data/hybrid_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs the class predicted for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4)\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/train/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/train\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "train_data_loader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes, clip_dir, proj_dir)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Training starts:')\n",
    "train_hybrid_attributor(hybrid_attributor, train_data_loader, 30, 0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
