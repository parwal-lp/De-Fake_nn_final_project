{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect and attribute these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "This project proposes methods to answer 2 of the research questions [RQ] proposed in the paper:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models (See sections 1 and 2)\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to the text-to-image model that generated it (See sections 3 and 4)\n",
    "\n",
    "<br>\n",
    "This notebook contains the instructions to reproduce the entire project from scratch, from the creation of the datasets to the design and training of the models.<br><br>\n",
    "\n",
    "For quick examples on how to use the implemented models see the __[tldrNotebook](tldr_notebook.ipynb)__, where the pre-built datasets and pre-trained weights can be downloaded.<br>\n",
    "\n",
    "For furhter informations the complete code of this project can be found in the source directory of the public GitHub repository __[Source Code](https://github.com/parwal-lp/De-Fake_nn_final_project/tree/main/src)__.\n",
    "\n",
    "Before proceeding to run the code in this Notebook, please read the instructions contained in the __[Readme](https://github.com/parwal-lp/De-Fake_nn_final_project/tree/main)__ on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reproduce this project from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the path variables to be used globally in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these paths as described below\n",
    "proj_dir = \"/home/parwal/Documents/test/De-Fake_nn_final_project\"    # set here the absolute path to the root of the current project (De-Fake)\n",
    "clip_dir = \"/home/parwal/Documents/GitHub/CLIP\"    # set here the absolute path to the CLIP directory cloned from GitHub\n",
    "ld_dir = \"/home/parwal/Documents/GitHub/latent-diffusion\"   # set here the absolute path to the LD directory cloned from GitHub\n",
    "glide_dir = \"/home/parwal/Documents/GitHub/glide-text2im\"   # set here the absolute path to the GLIDE directory cloned from GitHub\n",
    "\n",
    "SD_api_key = 'sk-6MTDQWuQSLiU3SIc8GEkQrFK7Yjh85JIj0nTfZZKFwircCQQ' # Set here your Stable Diffusion API key (with enough credit to generate at least 400 images\n",
    "\n",
    "# Do not change these paths, they are part of the implementation\n",
    "SD_generated_temp_dir = \"data/generated/SD+MSCOCO/\"\n",
    "GLIDE_generated_temp_dir = \"data/generated/GLIDE+MSCOCO/\"\n",
    "LD_generated_temp_dir = ld_dir + \"/outputs/txt2img-samples/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parwal/.local/lib/python3.8/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -- Declare all the imports needed in this notebook\n",
    "\n",
    "# External libraries imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# References to other files of this project\n",
    "# Functions for the management of data\n",
    "from src.data_collector import fetchImagesFromMSCOCO\n",
    "from src.dataset_generator import SD_generation, LD_generation, GLIDE_generation\n",
    "from src.format_dataset import format_dataset_binaryclass, formatIntoTrainTest, format_dataset_multiclass\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader\n",
    "# Functions for building and training the models\n",
    "from src.imageonly_detector.model import train_imageonly_detector, eval_imageonly_detector\n",
    "from src.imageonly_attributor.model import train_imageonly_attributor, eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, train_hybrid_detector, eval_hybrid_detector\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, train_hybrid_attributor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1. Detection of images generated by text-to-image generation models\n",
    "\n",
    "The study proposes two detector models:\n",
    "\n",
    "1. **Image-only detector**<br>binary classifier that decides whether an input image is fake or real.\n",
    "\n",
    "2. **Hybrid detector**<br>binary classifier that is able to tell if an image is fake or real, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only detector\n",
    "This model is implemented as a two-layer perceptron, to be used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "All the datasets are constitueted by a set of N real images (labeled 1), and a set of N corresponding fake generated images (labeled 0).\n",
    "\n",
    "Training (on a single dataset):\n",
    "- real images fetched from MSCOCO (class 1)\n",
    "- fake images generated by Stable Diffusion (SD) (class 0)\n",
    "\n",
    "Evaluation (on 3 different datasets):\n",
    "- real images always fetched from MSCOCO (class 1)\n",
    "- fake images generated respectively by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (class 0)\n",
    "\n",
    "The data is structured as follows:\n",
    "\n",
    "imageonly_detector_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by LD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fetch the real images together with their captions, for all the datasets described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO\n",
    "# The dataset generated using SD will be divided into train and test later on, it is temporarely saved in the directory fetched/MSCOCO_for_SD\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/fetched/MSCOCO_for_SD\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# real images and captions needed as input for the LD model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_LD/class_1\", \"data/imageonly_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "# real images and captions needed as input for the GLIDE model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_GLIDE/class_1\", \"data/imageonly_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously fetched captions to generate the fake images.<br><br>\n",
    "Notice that for SD we use the APIs, while for LD and GLIDE we used a downloaded local model.<br><br>\n",
    "Also, SD has a very strict protection against inappropriate text prompts, so it might refuse to process some of the prompts, even if they are legit.<br>\n",
    "This exception is handled by the implemented methods, and ignores the unprocessed images labeling them as invalid, those won't be part of the datasets, and even the real counterparts are not included in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/fetched/MSCOCO_for_SD/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "#use Latent Diffusion model to generate 50 images starting from the captions fetched before\n",
    "print(\"generating images using LD...\")\n",
    "LD_generation(\"data/imageonly_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "print(\"LD images generated successfully!\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#use GLIDE model to generate 50 images starting from the captions fetched before\n",
    "print(\"generating images using GLIDE...\")\n",
    "GLIDE_generation(\"data/imageonly_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "print(\"GLIDE images generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the collected and generated images from their respective folders into the dataset directory structured as described before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/fetched/MSCOCO_for_SD\", SD_generated_temp_dir, \"data/imageonly_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/imageonly_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/imageonly_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 1.1 (jpg images)<br>Each item of this dataset is transformed in order to achieve better performance.<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader obtained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 3.1605 Acc: 0.5400\n",
      "val Loss: 0.5465 Acc: 0.8061\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.4093 Acc: 0.8400\n",
      "val Loss: 1.2111 Acc: 0.6633\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.3474 Acc: 0.8600\n",
      "val Loss: 0.4639 Acc: 0.8061\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.5000 Acc: 0.8000\n",
      "val Loss: 0.3040 Acc: 0.8469\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.3316 Acc: 0.8600\n",
      "val Loss: 0.3483 Acc: 0.8776\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.2674 Acc: 0.9200\n",
      "val Loss: 0.2744 Acc: 0.8776\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.9000\n",
      "val Loss: 0.7405 Acc: 0.7755\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.3479 Acc: 0.8400\n",
      "val Loss: 0.4503 Acc: 0.8163\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.2347 Acc: 0.9000\n",
      "val Loss: 0.2777 Acc: 0.8673\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.5108 Acc: 0.8700\n",
      "val Loss: 0.2971 Acc: 0.8776\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.4476 Acc: 0.8200\n",
      "val Loss: 0.2558 Acc: 0.8980\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.2651 Acc: 0.9300\n",
      "val Loss: 0.2633 Acc: 0.8980\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.3327 Acc: 0.9200\n",
      "val Loss: 0.2528 Acc: 0.8776\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.2211 Acc: 0.9000\n",
      "val Loss: 0.2220 Acc: 0.9082\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.3183 Acc: 0.8900\n",
      "val Loss: 0.2111 Acc: 0.8980\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.1211 Acc: 0.9500\n",
      "val Loss: 0.2923 Acc: 0.8571\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.2460 Acc: 0.8900\n",
      "val Loss: 0.4651 Acc: 0.8163\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.3327 Acc: 0.8800\n",
      "val Loss: 0.2852 Acc: 0.8878\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.2233 Acc: 0.9400\n",
      "val Loss: 0.2860 Acc: 0.8571\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.2848 Acc: 0.8500\n",
      "val Loss: 0.2628 Acc: 0.8776\n",
      "\n",
      "Training complete in 0m 23s\n",
      "Best val Acc: 0.908163\n",
      "Evaluation starts\n",
      "loading model with trained weights...\n",
      "Evaluation on SD -> Acc: 0.8265 - Loss: 0.3302\n",
      "Evaluation on LD -> Acc: 0.7900 - Loss: 0.7290\n",
      "Evaluation on GLIDE -> Acc: 0.7700 - Loss: 0.5695\n",
      "Evaluation complete in 0m 1s\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = 'data/imageonly_detector_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts\")\n",
    "trained_model = train_imageonly_detector(model, dataloaders, dataset_sizes, num_epochs=20)\n",
    "\n",
    "# Load a model with the trained weights from the previous step, and evaluate it on test data\n",
    "print(\"Evaluation starts\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_detector(test_model, dataloaders, dataset_sizes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid detector\n",
    "For this problem we again implement a two-layer perceptron for binary classification, but in this case it will take as input not only the images but also their captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "\n",
    "The data is first fetched and generated in the exact same way as the dataset for the image-only detector.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- COLLECT REAL IMAGES FROM MSCOCO -------------------- #\n",
    "\n",
    "#SD+MSCOCO\n",
    "print(\"fetching images for SD...\")\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/hybrid_detector_data\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"fetching images for LD...\")\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_LD/class_1\", \"data/hybrid_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"fetching images for GLIDE...\")\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_GLIDE/class_1\", \"data/hybrid_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- GENERATE FAKE IMAGES USING SD, LD, GLIDE -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/hybrid_detector_data/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"generating images using LD...\")\n",
    "LD_generation(\"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "print(\"LD images generated successfully!\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"generating images using GLIDE...\")\n",
    "GLIDE_generation(\"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "print(\"GLIDE images generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FORMAT THE DATA INTO THE STRUCTURE NEEDED FOR TRAINING/TESTING -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "print(\"Building SD dataset...\")\n",
    "formatIntoTrainTest(\"data/fetched/MSCOCO_for_SD/\", SD_generated_temp_dir, \"data/hybrid_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"Building LD dataset...\")\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/hybrid_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"Building GLIDE dataset...\")\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/hybrid_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module.<br>A two-layer perceptron that outputs 0 (fake) or 1 (real) for each sample.<br><br>\n",
    "- **Create Dataset and DataLoader** starting from the row data fetched at 2.1 (jpg images and string captions).<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model.<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts\n",
      "EPOCH:  1/40  - MEAN ACCURACY:  tensor(0.5600)  - MEAN LOSS:  tensor(0.6888)\n",
      "EPOCH:  2/40  - MEAN ACCURACY:  tensor(0.5800)  - MEAN LOSS:  tensor(0.6864)\n",
      "EPOCH:  3/40  - MEAN ACCURACY:  tensor(0.6000)  - MEAN LOSS:  tensor(0.6840)\n",
      "EPOCH:  4/40  - MEAN ACCURACY:  tensor(0.6000)  - MEAN LOSS:  tensor(0.6817)\n",
      "EPOCH:  5/40  - MEAN ACCURACY:  tensor(0.6300)  - MEAN LOSS:  tensor(0.6794)\n",
      "EPOCH:  6/40  - MEAN ACCURACY:  tensor(0.6400)  - MEAN LOSS:  tensor(0.6771)\n",
      "EPOCH:  7/40  - MEAN ACCURACY:  tensor(0.6600)  - MEAN LOSS:  tensor(0.6748)\n",
      "EPOCH:  8/40  - MEAN ACCURACY:  tensor(0.6700)  - MEAN LOSS:  tensor(0.6727)\n",
      "EPOCH:  9/40  - MEAN ACCURACY:  tensor(0.7000)  - MEAN LOSS:  tensor(0.6703)\n",
      "EPOCH:  10/40  - MEAN ACCURACY:  tensor(0.7100)  - MEAN LOSS:  tensor(0.6682)\n",
      "EPOCH:  11/40  - MEAN ACCURACY:  tensor(0.7400)  - MEAN LOSS:  tensor(0.6658)\n",
      "EPOCH:  12/40  - MEAN ACCURACY:  tensor(0.7600)  - MEAN LOSS:  tensor(0.6634)\n",
      "EPOCH:  13/40  - MEAN ACCURACY:  tensor(0.7700)  - MEAN LOSS:  tensor(0.6611)\n",
      "EPOCH:  14/40  - MEAN ACCURACY:  tensor(0.7700)  - MEAN LOSS:  tensor(0.6588)\n",
      "EPOCH:  15/40  - MEAN ACCURACY:  tensor(0.7900)  - MEAN LOSS:  tensor(0.6566)\n",
      "EPOCH:  16/40  - MEAN ACCURACY:  tensor(0.8000)  - MEAN LOSS:  tensor(0.6540)\n",
      "EPOCH:  17/40  - MEAN ACCURACY:  tensor(0.8100)  - MEAN LOSS:  tensor(0.6518)\n",
      "EPOCH:  18/40  - MEAN ACCURACY:  tensor(0.8100)  - MEAN LOSS:  tensor(0.6494)\n",
      "EPOCH:  19/40  - MEAN ACCURACY:  tensor(0.8100)  - MEAN LOSS:  tensor(0.6470)\n",
      "EPOCH:  20/40  - MEAN ACCURACY:  tensor(0.8300)  - MEAN LOSS:  tensor(0.6444)\n",
      "EPOCH:  21/40  - MEAN ACCURACY:  tensor(0.8300)  - MEAN LOSS:  tensor(0.6421)\n",
      "EPOCH:  22/40  - MEAN ACCURACY:  tensor(0.8300)  - MEAN LOSS:  tensor(0.6394)\n",
      "EPOCH:  23/40  - MEAN ACCURACY:  tensor(0.8400)  - MEAN LOSS:  tensor(0.6368)\n",
      "EPOCH:  24/40  - MEAN ACCURACY:  tensor(0.8600)  - MEAN LOSS:  tensor(0.6346)\n",
      "EPOCH:  25/40  - MEAN ACCURACY:  tensor(0.8700)  - MEAN LOSS:  tensor(0.6318)\n",
      "EPOCH:  26/40  - MEAN ACCURACY:  tensor(0.8500)  - MEAN LOSS:  tensor(0.6291)\n",
      "EPOCH:  27/40  - MEAN ACCURACY:  tensor(0.8700)  - MEAN LOSS:  tensor(0.6263)\n",
      "EPOCH:  28/40  - MEAN ACCURACY:  tensor(0.8700)  - MEAN LOSS:  tensor(0.6235)\n",
      "EPOCH:  29/40  - MEAN ACCURACY:  tensor(0.8700)  - MEAN LOSS:  tensor(0.6207)\n",
      "EPOCH:  30/40  - MEAN ACCURACY:  tensor(0.8700)  - MEAN LOSS:  tensor(0.6177)\n",
      "EPOCH:  31/40  - MEAN ACCURACY:  tensor(0.8800)  - MEAN LOSS:  tensor(0.6150)\n",
      "EPOCH:  32/40  - MEAN ACCURACY:  tensor(0.8900)  - MEAN LOSS:  tensor(0.6123)\n",
      "EPOCH:  33/40  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.6092)\n",
      "EPOCH:  34/40  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.6061)\n",
      "EPOCH:  35/40  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.6030)\n",
      "EPOCH:  36/40  - MEAN ACCURACY:  tensor(0.9200)  - MEAN LOSS:  tensor(0.5998)\n",
      "EPOCH:  37/40  - MEAN ACCURACY:  tensor(0.9100)  - MEAN LOSS:  tensor(0.5968)\n",
      "EPOCH:  38/40  - MEAN ACCURACY:  tensor(0.9100)  - MEAN LOSS:  tensor(0.5937)\n",
      "EPOCH:  39/40  - MEAN ACCURACY:  tensor(0.9100)  - MEAN LOSS:  tensor(0.5905)\n",
      "EPOCH:  40/40  - MEAN ACCURACY:  tensor(0.9100)  - MEAN LOSS:  tensor(0.5872)\n"
     ]
    }
   ],
   "source": [
    "#Build the model\n",
    "print('Building the model...')\n",
    "hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "\n",
    "#Build the dataset\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_detector_data/mscoco_captions.csv\"\n",
    "real_img_dir = \"data/hybrid_detector_data/train/class_1\"\n",
    "fake_img_dir = \"data/hybrid_detector_data/train/class_0\"\n",
    "train_data_loader = get_dataset_loader(captions_file, real_img_dir, fake_img_dir, clip_dir, proj_dir)\n",
    "\n",
    "#Train the model\n",
    "print('Training starts')\n",
    "train_hybrid_detector(hybrid_detector, train_data_loader, 40, 0.0009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it on some test datasets.<br>\n",
    "In particular we will evaluate it on:<br>\n",
    "- Stable Diffusion (SD), dataset generated from the same image-to-text generator used for the train dataset.\n",
    "- GLIDE\n",
    "- Latent Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model with trained weights...\n",
      "Evaluation starts\n",
      "Evaluation on SD --> Accuracy: 0.8666667342185974 - Loss: 0.6129067540168762\n",
      "Evaluation on GLIDE --> Accuracy: 0.7199999690055847 - Loss: 0.6579972505569458\n",
      "Evaluation on LD --> Accuracy: 0.800000011920929 - Loss: 0.6495194435119629\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the weights we trained in the previous code block\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'], clip_dir, proj_dir)\n",
    "    SDloss, SDacc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {SDacc} - Loss: {SDloss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2. Attribution of the fake images to their source model\n",
    "\n",
    "The study proposes two attributor models:\n",
    "\n",
    "1. **Image-only attributor**<br>multi-class classifier that assigns each input image to its source generation model, given the image only.\n",
    "\n",
    "2. **Hybrid attributor**<br>multi-class classifier that assigns each input image to its source generation model, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only attributor\n",
    "\n",
    "In this section we will build and train a model that is able to assign an image to the model that generated it, given only that image.<br><br>\n",
    "The classes that this model will be able to address are the following:\n",
    "- real image -> class 0\n",
    "- fake image generated by SD -> class 1\n",
    "- fake image generated by LD -> class 2\n",
    "- fake image generated by GLIDE -> class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "\n",
    "We generate two datasets, one for training and one for evaluating the model.<br>\n",
    "The steps needed to generate the two datasets are the same:\n",
    "- fetch real images and their captions from MSCOCO (class 0)\n",
    "- generate fake images with SD using the captions of the real images (class 1)\n",
    "- generate fake images with LD using the captions of the real images (class 2)\n",
    "- generate fake images with GLIDE using the captions of the real images (class 3)\n",
    "- move the real and generated images into a dataset directory, with the following structure:\n",
    "\n",
    "imageonly_attributor_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_real/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_SD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by GLIDE*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by LD*<br>\n",
    "&emsp;&emsp;├── test/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sposto le immagini LD\n",
      "sposto le immagini SD\n",
      "['84610.jpg', '331551.jpg', '434986.jpg', '89589.jpg', '165835.jpg', '264884.jpg', '471335.jpg', '391519.jpg', '117466.jpg', '29715.jpg', '564204.jpg', '27157.jpg']\n",
      "sposto le immagini GLIDE\n"
     ]
    }
   ],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/train/class_real\", \"data/imageonly_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sposto le immagini LD\n",
      "sposto le immagini SD\n",
      "['84610.jpg', '331551.jpg', '434986.jpg', '89589.jpg', '516774.jpg', '165835.jpg', '264884.jpg', '471335.jpg', '391519.jpg', '117466.jpg', '29715.jpg', '564204.jpg', '451012.jpg', '27157.jpg']\n",
      "sposto le immagini GLIDE\n"
     ]
    }
   ],
   "source": [
    "# Repeat the same procedure for the test dataset\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/test/class_real\", \"data/imageonly_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 1.1 (jpg images)<br>Each item of this dataset is transformed in order to obtain better generalization<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 6.1191 Acc: 0.1960\n",
      "test Loss: 4.4390 Acc: 0.3687\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 2.6933 Acc: 0.5477\n",
      "test Loss: 2.2395 Acc: 0.5657\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 1.3284 Acc: 0.7538\n",
      "test Loss: 1.4689 Acc: 0.6515\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 0.9205 Acc: 0.7789\n",
      "test Loss: 1.0956 Acc: 0.7374\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 0.9387 Acc: 0.7789\n",
      "test Loss: 1.8423 Acc: 0.6162\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 0.6712 Acc: 0.8291\n",
      "test Loss: 1.1617 Acc: 0.6818\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 0.4897 Acc: 0.8543\n",
      "test Loss: 1.4001 Acc: 0.6212\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 0.7176 Acc: 0.7940\n",
      "test Loss: 1.6833 Acc: 0.6465\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 0.7309 Acc: 0.8442\n",
      "test Loss: 1.0576 Acc: 0.7424\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 0.4961 Acc: 0.8291\n",
      "test Loss: 0.8649 Acc: 0.7475\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 0.4871 Acc: 0.8241\n",
      "test Loss: 0.8571 Acc: 0.7576\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 0.3868 Acc: 0.8894\n",
      "test Loss: 0.8762 Acc: 0.7222\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.3499 Acc: 0.8844\n",
      "test Loss: 0.7824 Acc: 0.7576\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.3660 Acc: 0.8794\n",
      "test Loss: 0.8192 Acc: 0.7727\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.3323 Acc: 0.8794\n",
      "test Loss: 0.9961 Acc: 0.7323\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.3627 Acc: 0.8894\n",
      "test Loss: 1.0262 Acc: 0.7424\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.4941 Acc: 0.8291\n",
      "test Loss: 1.0531 Acc: 0.7071\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.4409 Acc: 0.8744\n",
      "test Loss: 1.0652 Acc: 0.6869\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.5162 Acc: 0.8241\n",
      "test Loss: 1.1111 Acc: 0.6566\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.3637 Acc: 0.8744\n",
      "test Loss: 1.0199 Acc: 0.7121\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.4933 Acc: 0.8593\n",
      "test Loss: 0.6507 Acc: 0.7727\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.3722 Acc: 0.8693\n",
      "test Loss: 0.9688 Acc: 0.7323\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.4789 Acc: 0.8392\n",
      "test Loss: 0.9129 Acc: 0.7071\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.4752 Acc: 0.8191\n",
      "test Loss: 1.0352 Acc: 0.7020\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.2468 Acc: 0.9246\n",
      "test Loss: 0.7413 Acc: 0.7677\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.3203 Acc: 0.9045\n",
      "test Loss: 0.7477 Acc: 0.7172\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.2553 Acc: 0.9095\n",
      "test Loss: 0.7806 Acc: 0.7424\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.4086 Acc: 0.8844\n",
      "test Loss: 0.8388 Acc: 0.7121\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.1775 Acc: 0.9296\n",
      "test Loss: 1.0870 Acc: 0.6566\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.1626 Acc: 0.9447\n",
      "test Loss: 0.8750 Acc: 0.7424\n",
      "Best val Acc: 0.772727\n",
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "Evaluation results -> ACC: 0.7727 - LOSS: 0.8192\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts:\")\n",
    "trained_model = train_imageonly_attributor(model, dataloaders, dataset_sizes, num_epochs=30)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_attributor.pth'))\n",
    "eval_imageonly_attributor(test_model, dataloaders, dataset_sizes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid attributor\n",
    "In this section we will build and train a model similar to the model built in section 1.<br>\n",
    "The difference is that instead of taking as input only the image, this model also considers its textual caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test datasets are generated in the same way as in the image-only attributor case.<br>\n",
    "For the dataset directory structure also refer to the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sposto le immagini LD\n",
      "sposto le immagini SD\n",
      "['84610.jpg', '331551.jpg', '333946.jpg', '434986.jpg', '89589.jpg', '516774.jpg', '165835.jpg', '264884.jpg', '471335.jpg', '391519.jpg', '117466.jpg', '29715.jpg', '564204.jpg', '451012.jpg', '27157.jpg']\n",
      "sposto le immagini GLIDE\n"
     ]
    }
   ],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/train/class_real\", \"data/hybrid_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then denerate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/test/class_real\", \"data/hybrid_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs the class predicted for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "EPOCH:  1/30  - MEAN ACCURACY:  tensor(0.2833)  - MEAN LOSS:  tensor(1.3881)\n",
      "EPOCH:  2/30  - MEAN ACCURACY:  tensor(0.4417)  - MEAN LOSS:  tensor(1.3570)\n",
      "EPOCH:  3/30  - MEAN ACCURACY:  tensor(0.5217)  - MEAN LOSS:  tensor(1.3271)\n",
      "EPOCH:  4/30  - MEAN ACCURACY:  tensor(0.6200)  - MEAN LOSS:  tensor(1.2942)\n",
      "EPOCH:  5/30  - MEAN ACCURACY:  tensor(0.6300)  - MEAN LOSS:  tensor(1.2596)\n",
      "EPOCH:  6/30  - MEAN ACCURACY:  tensor(0.7300)  - MEAN LOSS:  tensor(1.2201)\n",
      "EPOCH:  7/30  - MEAN ACCURACY:  tensor(0.8017)  - MEAN LOSS:  tensor(1.1798)\n",
      "EPOCH:  8/30  - MEAN ACCURACY:  tensor(0.8200)  - MEAN LOSS:  tensor(1.1288)\n",
      "EPOCH:  9/30  - MEAN ACCURACY:  tensor(0.7967)  - MEAN LOSS:  tensor(1.0886)\n",
      "EPOCH:  10/30  - MEAN ACCURACY:  tensor(0.8600)  - MEAN LOSS:  tensor(1.0332)\n",
      "EPOCH:  11/30  - MEAN ACCURACY:  tensor(0.8867)  - MEAN LOSS:  tensor(0.9833)\n",
      "EPOCH:  12/30  - MEAN ACCURACY:  tensor(0.8800)  - MEAN LOSS:  tensor(0.9266)\n",
      "EPOCH:  13/30  - MEAN ACCURACY:  tensor(0.8850)  - MEAN LOSS:  tensor(0.8758)\n",
      "EPOCH:  14/30  - MEAN ACCURACY:  tensor(0.9100)  - MEAN LOSS:  tensor(0.8165)\n",
      "EPOCH:  15/30  - MEAN ACCURACY:  tensor(0.9150)  - MEAN LOSS:  tensor(0.7698)\n",
      "EPOCH:  16/30  - MEAN ACCURACY:  tensor(0.8983)  - MEAN LOSS:  tensor(0.7226)\n",
      "EPOCH:  17/30  - MEAN ACCURACY:  tensor(0.9167)  - MEAN LOSS:  tensor(0.6766)\n",
      "EPOCH:  18/30  - MEAN ACCURACY:  tensor(0.9350)  - MEAN LOSS:  tensor(0.6266)\n",
      "EPOCH:  19/30  - MEAN ACCURACY:  tensor(0.9500)  - MEAN LOSS:  tensor(0.5821)\n",
      "EPOCH:  20/30  - MEAN ACCURACY:  tensor(0.9317)  - MEAN LOSS:  tensor(0.5479)\n",
      "EPOCH:  21/30  - MEAN ACCURACY:  tensor(0.9650)  - MEAN LOSS:  tensor(0.5072)\n",
      "EPOCH:  22/30  - MEAN ACCURACY:  tensor(0.9450)  - MEAN LOSS:  tensor(0.4751)\n",
      "EPOCH:  23/30  - MEAN ACCURACY:  tensor(0.9600)  - MEAN LOSS:  tensor(0.4411)\n",
      "EPOCH:  24/30  - MEAN ACCURACY:  tensor(0.9500)  - MEAN LOSS:  tensor(0.4109)\n",
      "EPOCH:  25/30  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3857)\n",
      "EPOCH:  26/30  - MEAN ACCURACY:  tensor(0.9700)  - MEAN LOSS:  tensor(0.3659)\n",
      "EPOCH:  27/30  - MEAN ACCURACY:  tensor(0.9700)  - MEAN LOSS:  tensor(0.3372)\n",
      "EPOCH:  28/30  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3201)\n",
      "EPOCH:  29/30  - MEAN ACCURACY:  tensor(0.9900)  - MEAN LOSS:  tensor(0.2977)\n",
      "EPOCH:  30/30  - MEAN ACCURACY:  tensor(0.9850)  - MEAN LOSS:  tensor(0.2830)\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4)\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/train/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/train\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "train_data_loader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes, clip_dir, proj_dir)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Training starts:')\n",
    "train_hybrid_attributor(hybrid_attributor, train_data_loader, 30, 0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
