{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect and attribute these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "Therefore, this research proposes methods to answer the following 3 research questions [RQ]:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models (See sections 1 and 2)\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to the text-to-image model that generated it (See sections 3 and 4)\n",
    "\n",
    "- **RQ3**. Analysis of the likelihood that different text prompts have to generate authentic images\n",
    "\n",
    "<br>\n",
    "This notebook contains the instructions to reproduce the entire project from scratch, from the creation of the datasets to the design and training of the models.<br><br>\n",
    "\n",
    "For examples on how to use the implemented models see the __[tldrNotebook](tldr_notebook.ipynb)__, where the pre-built datasets and pre-trained weights can be downloaded.<br>\n",
    "\n",
    "For furhter informations the complete code of this project can be found in the source directory of the public GitHub repository __[Source Code](https://github.com/parwal-lp/De-Fake_nn_final_project/src)__.\n",
    "\n",
    "Before proceeding to run the code in this Notebook, please read the instructions contained in the __[Readme](https://github.com/parwal-lp/De-Fake_nn_final_project/tree/main)__ on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reproduce this project from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the path variables to be used globally in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these paths as described below\n",
    "proj_dir = \"/home/parwal/Documents/test/De-Fake_nn_final_project\"    # set here the absolute path to the root of the current project (De-Fake)\n",
    "clip_dir = \"/home/parwal/Documents/GitHub/CLIP\"    # set here the absolute path to the CLIP directory cloned from GitHub\n",
    "ld_dir = \"/home/parwal/Documents/GitHub/latent-diffusion\"   # set here the absolute path to the LD directory cloned from GitHub\n",
    "glide_dir = \"/home/parwal/Documents/GitHub/glide-text2im\"   # set here the absolute path to the GLIDE directory cloned from GitHub\n",
    "\n",
    "SD_api_key = 'sk-J3XHM0kic6LY184fFrv8vgJTswO9pommzWo9GpDKMnnprUT3' # Set here your Stable Diffusion API key (with enough credit to generate at least 100 images, the inital free 25 credits should be sufficient)\n",
    "\n",
    "# Do not change these paths, they are part of the implementation\n",
    "SD_generated_temp_dir = \"data/generated/SD+MSCOCO/\"\n",
    "GLIDE_generated_temp_dir = \"data/generated/GLIDE+MSCOCO/\"\n",
    "LD_generated_temp_dir = ld_dir + \"outputs/txt2img-samples/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare all the imports needed in this notebook\n",
    "\n",
    "# External libraries imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# References to other files of this project\n",
    "# Functions for the management of data\n",
    "from src.data_collector import fetchImagesFromMSCOCO\n",
    "from src.dataset_generator import SD_generation, LD_generation, GLIDE_generation\n",
    "from src.format_dataset import format_dataset_binaryclass, formatIntoTrainTest, format_dataset_multiclass\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader\n",
    "# Functions for building and training the models\n",
    "from src.imageonly_detector.model import train_imageonly_detector, eval_imageonly_detector\n",
    "from src.imageonly_attributor.model import train_imageonly_attributor, eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, train_hybrid_detector, eval_hybrid_detector\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, train_hybrid_attributor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1. Detection of images generated by text-to-image generation models\n",
    "\n",
    "The study proposes two detector models:\n",
    "\n",
    "1. **Image-only detector**<br>binary classifier that decides whether an input image is fake or real.\n",
    "\n",
    "2. **Hybrid detector**<br>binary classifier that is able to tell if an image is fake or real, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only detector\n",
    "This model is implemented as a two-layer perceptron, to be used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "All the datasets are constitueted by a set of N real images (labeled 1), and a set of N corresponding fake generated images (labeled 0).\n",
    "\n",
    "Training (on a single dataset):\n",
    "- real images fetched from MSCOCO (class 1)\n",
    "- fake images generated by Stable Diffusion (SD) (class 0)\n",
    "\n",
    "Evaluation (on 3 different datasets):\n",
    "- real images always fetched from MSCOCO (class 1)\n",
    "- fake images generated respectively by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (class 0)\n",
    "\n",
    "The data is structured as follows:\n",
    "\n",
    "imageonly_detector_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by LD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fetch the real images together with their captions, for all the datasets described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.63s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=12.22s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=12.45s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.76s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# sys.path.insert(10, '/home/parwal/Documents/GitHub/De-Fake_nn_final_project/src/imageonly_detector')\n",
    "\n",
    "#SD+MSCOCO\n",
    "# The dataset generated using SD will be divided into train and test later on, it is temporarely saved in the directory fetched/MSCOCO_for_SD\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/fetched/MSCOCO_for_SD\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# real images and captions needed as input for the LD model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_LD/class_1\", \"data/imageonly_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "# real images and captions needed as input for the GLIDE model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_GLIDE/class_1\", \"data/imageonly_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously fetched captions to generate the fake images.<br><br>\n",
    "Notice that for SD we use the APIs, while for LD and GLIDE we used a downloaded local model.<br><br>\n",
    "Also, SD has a very strict protection against inappropriate text prompts, so it might refuse to process some of the prompts, even if they are legit.<br>\n",
    "This exception is handled by the implemented methods, and ignores the unprocessed images labeling them as invalid, those won't be part of the datasets, and even the real counterparts are not included in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/fetched/MSCOCO_for_SD/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "#use Latent Diffusion model to generate 50 images starting from the captions fetched before\n",
    "print(\"generating images using LD...\")\n",
    "LD_generation(\"data/imageonly_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "print(\"LD images generated successfully!\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#use GLIDE model to generate 50 images starting from the captions fetched before\n",
    "print(\"generating images using GLIDE...\")\n",
    "GLIDE_generation(\"data/imageonly_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "print(\"GLIDE images generated successfully!\")\n",
    "\n",
    "# TODO testare sono arrivata qui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the collected and generated images from their respective folders into the dataset directory structured as described before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the collected data in the previously described structure\n",
    "\n",
    "# TODO TEST\n",
    "\n",
    "#mitico1996@libero.it\n",
    "#valeriocantelmo@libero.it\n",
    "#valeriocantelmo@gmail.com\n",
    "\n",
    "#SD+MSCOCO\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/MSCOCO_for_SD/images\", SD_generated_temp_dir, \"data/imageonly_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/imageonly_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/imageonly_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images)<br>Each item of this dataset is transformed -- TO DO WHY TRANSFORM CAPIRE --<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 3.1478 Acc: 0.5100\n",
      "val Loss: 0.4255 Acc: 0.8265\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.5736 Acc: 0.8400\n",
      "val Loss: 0.2364 Acc: 0.9388\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.6277 Acc: 0.8300\n",
      "val Loss: 0.4284 Acc: 0.8571\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.5500 Acc: 0.8400\n",
      "val Loss: 0.5303 Acc: 0.8469\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.7450 Acc: 0.8100\n",
      "val Loss: 1.0299 Acc: 0.7143\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.6322 Acc: 0.8100\n",
      "val Loss: 0.3712 Acc: 0.8673\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.2620 Acc: 0.8900\n",
      "val Loss: 0.6136 Acc: 0.8469\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.5283 Acc: 0.8300\n",
      "val Loss: 0.3662 Acc: 0.8571\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.4382 Acc: 0.8400\n",
      "val Loss: 0.3725 Acc: 0.8571\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.2583 Acc: 0.9100\n",
      "val Loss: 0.3593 Acc: 0.8776\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.4544 Acc: 0.8700\n",
      "val Loss: 0.3310 Acc: 0.8265\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.3764 Acc: 0.8700\n",
      "val Loss: 0.2749 Acc: 0.8673\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.2885 Acc: 0.8900\n",
      "val Loss: 0.3027 Acc: 0.8878\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.1382 Acc: 0.9500\n",
      "val Loss: 0.2945 Acc: 0.8776\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.3013 Acc: 0.8900\n",
      "val Loss: 0.2815 Acc: 0.8980\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.3444 Acc: 0.8700\n",
      "val Loss: 0.2924 Acc: 0.8673\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.3577 Acc: 0.8500\n",
      "val Loss: 0.2903 Acc: 0.8673\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.2179 Acc: 0.9300\n",
      "val Loss: 0.3183 Acc: 0.8673\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.2132 Acc: 0.9300\n",
      "val Loss: 0.3012 Acc: 0.8571\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.2730 Acc: 0.8800\n",
      "val Loss: 0.2471 Acc: 0.8776\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.1143 Acc: 0.9700\n",
      "val Loss: 0.2833 Acc: 0.8571\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.3795 Acc: 0.8800\n",
      "val Loss: 0.2984 Acc: 0.8776\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4218 Acc: 0.8400\n",
      "val Loss: 0.2528 Acc: 0.8776\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0951 Acc: 0.9800\n",
      "val Loss: 0.2776 Acc: 0.8776\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.2388 Acc: 0.8900\n",
      "val Loss: 0.2753 Acc: 0.8776\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.2440 Acc: 0.8900\n",
      "val Loss: 0.2767 Acc: 0.8776\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.4718 Acc: 0.8200\n",
      "val Loss: 0.2823 Acc: 0.8776\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.3393 Acc: 0.8900\n",
      "val Loss: 0.2696 Acc: 0.8776\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.3162 Acc: 0.9200\n",
      "val Loss: 0.2740 Acc: 0.8776\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.2343 Acc: 0.8900\n",
      "val Loss: 0.2806 Acc: 0.8776\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.2457 Acc: 0.9000\n",
      "val Loss: 0.3151 Acc: 0.8878\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3116 Acc: 0.8900\n",
      "val Loss: 0.2711 Acc: 0.8776\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.2606 Acc: 0.9300\n",
      "val Loss: 0.2856 Acc: 0.8878\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.2339 Acc: 0.9000\n",
      "val Loss: 0.2874 Acc: 0.8776\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.3152 Acc: 0.8800\n",
      "val Loss: 0.2676 Acc: 0.8878\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.3629 Acc: 0.8500\n",
      "val Loss: 0.2823 Acc: 0.8776\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.1239 Acc: 0.9300\n",
      "val Loss: 0.2778 Acc: 0.8673\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.2192 Acc: 0.8800\n",
      "val Loss: 0.3060 Acc: 0.8776\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.2472 Acc: 0.8900\n",
      "val Loss: 0.3023 Acc: 0.8776\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.9600\n",
      "val Loss: 0.3184 Acc: 0.8776\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.3029 Acc: 0.8700\n",
      "val Loss: 0.3159 Acc: 0.8673\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.2238 Acc: 0.8900\n",
      "val Loss: 0.2747 Acc: 0.8776\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.1823 Acc: 0.9100\n",
      "val Loss: 0.2980 Acc: 0.8776\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.3327 Acc: 0.9000\n",
      "val Loss: 0.2975 Acc: 0.8776\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.2853 Acc: 0.9000\n",
      "val Loss: 0.2738 Acc: 0.8878\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.1707 Acc: 0.9200\n",
      "val Loss: 0.2839 Acc: 0.8776\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.2591 Acc: 0.9100\n",
      "val Loss: 0.2679 Acc: 0.8980\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.1952 Acc: 0.9200\n",
      "val Loss: 0.2745 Acc: 0.8776\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.3737 Acc: 0.8500\n",
      "val Loss: 0.3033 Acc: 0.8878\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.1844 Acc: 0.9400\n",
      "val Loss: 0.2911 Acc: 0.8776\n",
      "\n",
      "Training complete in 0m 47s\n",
      "Best val Acc: 0.938776\n",
      "Evaluation starts\n",
      "loading model with trained weights...\n",
      "val_LD Loss: 1.4511 Acc: 0.6800\n",
      "val_GLIDE Loss: 1.1783 Acc: 0.7000\n",
      "Evaluation complete in 0m 1s\n"
     ]
    }
   ],
   "source": [
    " # Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_LD': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_GLIDE': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_detector_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts\")\n",
    "trained_model = train_imageonly_detector(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_detector(test_model, dataloaders, dataset_sizes) #valuta se serve, in fondo il test lo fai durante il training... viene uguale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid detector\n",
    "For this problem we again implement a two-layer perceptron for binary classification, but in this case it will take as input not only the images but also their captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "\n",
    "The data is first fetched and generated in the exact same way as the dataset for the image-only detector.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- COLLECT REAL IMAGES FROM MSCOCO -------------------- #\n",
    "\n",
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO\n",
    "fetchImagesFromMSCOCO(\"data/MSCOCO_for_SD_hybrid/images\", \"data/MSCOCO_for_SD_hybrid\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_LD/class_1\", \"data/hybrid_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_GLIDE/class_1\", \"data/hybrid_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- GENERATE FAKE IMAGES USING SD, LD, GLIDE -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "#%run src/imageonly_detector/SD_MSCOCO_data_generation.py\n",
    "SD_generation(\"data/MSCOCO_for_SD_hybrid/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# N.B.\n",
    "# prima di lanciare questo comando, aggiungere il file src/imageonly_detector/txt2img_batch.py alla directory latent-diffusion/scripts/\n",
    "#resetto la directory corrente a quella del progetto de-fake, altrimenti il file da eseguire non viene trovato\n",
    "#questo è necessario perché LD_MSCOCO_data_generation.py cambia la directory a quella di latent-diffusion\n",
    "#os.chdir(\"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\")\n",
    "#%run src/imageonly_detector/LD_MSCOCO_data_generation_batch.py\n",
    "LD_generation(\"data/hybrid_detector_data/val_LD/mscoco_captions.csv\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#NON HO MAI PROVATO A RUNNARLO, altrimenti rigenera il modello (3gb)\n",
    "#provare a runnarlo proprio alla fine di tutto per sicurezza\n",
    "#%run src/imageonly_detector/GLIDE_MSCOCO_data_generation.ipynb #TODO\n",
    "GLIDE_generation(\"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FORMAT THE DATA INTO THE STRUCTURE NEEDED FOR TRAINING/TESTING -------------------- #\n",
    "os.chdir(\"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\")\n",
    "\n",
    "#transform the collected data in the previously described structure\n",
    "\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/MSCOCO_for_SD_hybrid/images\", SD_generated_temp_dir, \"data/hybrid_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/hybrid_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/hybrid_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs 0 (fake) or 1 (real) for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "EPOCH:  1/10  - MEAN ACCURACY:  tensor(0.5000)  - MEAN LOSS:  tensor(0.6976)\n",
      "EPOCH:  2/10  - MEAN ACCURACY:  tensor(0.5400)  - MEAN LOSS:  tensor(0.6853)\n",
      "EPOCH:  3/10  - MEAN ACCURACY:  tensor(0.5900)  - MEAN LOSS:  tensor(0.6740)\n",
      "EPOCH:  4/10  - MEAN ACCURACY:  tensor(0.6200)  - MEAN LOSS:  tensor(0.6629)\n",
      "EPOCH:  5/10  - MEAN ACCURACY:  tensor(0.7800)  - MEAN LOSS:  tensor(0.6523)\n",
      "EPOCH:  6/10  - MEAN ACCURACY:  tensor(0.8200)  - MEAN LOSS:  tensor(0.6393)\n",
      "EPOCH:  7/10  - MEAN ACCURACY:  tensor(0.8700)  - MEAN LOSS:  tensor(0.6267)\n",
      "EPOCH:  8/10  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.6138)\n",
      "EPOCH:  9/10  - MEAN ACCURACY:  tensor(0.8900)  - MEAN LOSS:  tensor(0.5987)\n",
      "EPOCH:  10/10  - MEAN ACCURACY:  tensor(0.9300)  - MEAN LOSS:  tensor(0.5838)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Build the model\n",
    "print('Building the model...')\n",
    "hybrid_detector = TwoLayerPerceptron(1024, 100, 1).to(device)\n",
    "\n",
    "#Build the dataset\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_detector_data/mscoco_captions.csv\"\n",
    "real_img_dir = \"data/hybrid_detector_data/train/class_1\"\n",
    "fake_img_dir = \"data/hybrid_detector_data/train/class_0\"\n",
    "train_data_loader = get_dataset_loader(captions_file, real_img_dir, fake_img_dir)\n",
    "\n",
    "#Train the model\n",
    "print('Training starts')\n",
    "train_hybrid_detector(hybrid_detector, train_data_loader, 10, 0.005) # few epochs are needed because we use transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it on some test datasets.<br>\n",
    "In particular we will evaluate it on:<br>\n",
    "- Stable Diffusion (SD), dataset generated from the same image-to-text generator used for the train dataset.\n",
    "- GLIDE\n",
    "- Latent Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "testing...\n",
      "Evaluation on SD --> Accuracy: 0.8933333158493042 - Loss: 0.5959395170211792\n",
      "Evaluation on GLIDE --> Accuracy: 0.7100000381469727 - Loss: 0.6429540514945984\n",
      "Evaluation on LD --> Accuracy: 0.7100000977516174 - Loss: 0.6497437357902527\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the weights we trained in the previous code block\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'])\n",
    "    SDloss, SDacc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {SDacc} - Loss: {SDloss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2. Attribution of the fake images to their source model\n",
    "\n",
    "The study proposes two attributor models:\n",
    "\n",
    "1. **Image-only attributor**<br>multi-class classifier that assigns each input image to its source generation model, given the image only.\n",
    "\n",
    "2. **Hybrid attributor**<br>multi-class classifier that assigns each input image to its source generation model, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only attributor\n",
    "\n",
    "In this section we will build and train a model that is able to assign an image to the model that generated it, given only that image.<br><br>\n",
    "The classes that this model will be able to address are the following:\n",
    "- real image -> class 0\n",
    "- fake image generated by SD -> class 1\n",
    "- fake image generated by LD -> class 2\n",
    "- fake image generated by GLIDE -> class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "\n",
    "We generate two datasets, one for training and one for evaluating the model.<br>\n",
    "The steps needed to generate the two datasets are the same:\n",
    "- fetch real images and their captions from MSCOCO (class 0)\n",
    "- generate fake images with SD using the captions of the real images (class 1)\n",
    "- generate fake images with LD using the captions of the real images (class 2)\n",
    "- generate fake images with GLIDE using the captions of the real images (class 3)\n",
    "- move the real and generated images into a dataset directory, with the following structure:\n",
    "\n",
    "imageonly_attributor_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_real/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_SD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by GLIDE*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by LD*<br>\n",
    "&emsp;&emsp;├── test/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(proj_dir)\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/train/class_real\", \"data/imageonly_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same procedure for the test dataset\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/test/class_real\", \"data/imageonly_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images)<br>Each item of this dataset is transformed -- TO DO WHY TRANSFORM CAPIRE --<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 6.2548 Acc: 0.1800\n",
      "test Loss: 7.6032 Acc: 0.2941\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.3690 Acc: 0.6400\n",
      "test Loss: 2.6208 Acc: 0.5913\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.3048 Acc: 0.7300\n",
      "test Loss: 2.3382 Acc: 0.6192\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.2219 Acc: 0.7150\n",
      "test Loss: 1.6931 Acc: 0.6254\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.9729 Acc: 0.7700\n",
      "test Loss: 1.4934 Acc: 0.7368\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.6812 Acc: 0.8000\n",
      "test Loss: 1.3606 Acc: 0.7152\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.6774 Acc: 0.8050\n",
      "test Loss: 1.8063 Acc: 0.6656\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.7800\n",
      "test Loss: 1.1123 Acc: 0.6873\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.7269 Acc: 0.7350\n",
      "test Loss: 1.4779 Acc: 0.6749\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.7021 Acc: 0.8050\n",
      "test Loss: 1.2167 Acc: 0.7183\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.4763 Acc: 0.8500\n",
      "test Loss: 1.4460 Acc: 0.6378\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.4252 Acc: 0.8600\n",
      "test Loss: 1.6389 Acc: 0.5944\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.5537 Acc: 0.8750\n",
      "test Loss: 1.3400 Acc: 0.6533\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.7066 Acc: 0.7500\n",
      "test Loss: 1.8597 Acc: 0.6347\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.4604 Acc: 0.8250\n",
      "test Loss: 1.6523 Acc: 0.6842\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.4781 Acc: 0.8550\n",
      "test Loss: 1.0601 Acc: 0.7368\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.5318 Acc: 0.8400\n",
      "test Loss: 1.2168 Acc: 0.6749\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.3550 Acc: 0.8700\n",
      "test Loss: 0.9142 Acc: 0.7585\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.3585 Acc: 0.8850\n",
      "test Loss: 0.9164 Acc: 0.7430\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.3645 Acc: 0.8650\n",
      "test Loss: 1.0482 Acc: 0.7059\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.3516 Acc: 0.8950\n",
      "test Loss: 0.8064 Acc: 0.7368\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.2266 Acc: 0.9250\n",
      "test Loss: 1.1621 Acc: 0.6842\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.2775 Acc: 0.9200\n",
      "test Loss: 1.1644 Acc: 0.6780\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.3884 Acc: 0.8600\n",
      "test Loss: 1.3457 Acc: 0.6656\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.4411 Acc: 0.8750\n",
      "test Loss: 1.3577 Acc: 0.6378\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.2313 Acc: 0.9350\n",
      "test Loss: 1.6289 Acc: 0.6068\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.5093 Acc: 0.8300\n",
      "test Loss: 1.2703 Acc: 0.6037\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.3359 Acc: 0.8800\n",
      "test Loss: 1.6213 Acc: 0.5944\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.3488 Acc: 0.8600\n",
      "test Loss: 1.6734 Acc: 0.6471\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.2923 Acc: 0.9000\n",
      "test Loss: 1.1228 Acc: 0.6687\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.2598 Acc: 0.9050\n",
      "test Loss: 1.0454 Acc: 0.6873\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3127 Acc: 0.9000\n",
      "test Loss: 1.0483 Acc: 0.6811\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.4651 Acc: 0.8450\n",
      "test Loss: 1.3293 Acc: 0.6502\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.5153 Acc: 0.8550\n",
      "test Loss: 1.2866 Acc: 0.5913\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.2835 Acc: 0.9200\n",
      "test Loss: 0.9661 Acc: 0.6966\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.2670 Acc: 0.8950\n",
      "test Loss: 0.9760 Acc: 0.7245\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.4408 Acc: 0.8250\n",
      "test Loss: 1.1373 Acc: 0.6533\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.3756 Acc: 0.8950\n",
      "test Loss: 0.9749 Acc: 0.7090\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.3103 Acc: 0.8900\n",
      "test Loss: 0.9738 Acc: 0.7152\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2774 Acc: 0.9200\n",
      "test Loss: 1.1538 Acc: 0.6749\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.2443 Acc: 0.9050\n",
      "test Loss: 1.2308 Acc: 0.6687\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.1548 Acc: 0.9550\n",
      "test Loss: 1.2738 Acc: 0.6594\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.3822 Acc: 0.8600\n",
      "test Loss: 1.1694 Acc: 0.6502\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.3765 Acc: 0.8750\n",
      "test Loss: 1.1531 Acc: 0.6533\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.2734 Acc: 0.9300\n",
      "test Loss: 1.0459 Acc: 0.6780\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.1911 Acc: 0.9250\n",
      "test Loss: 1.2940 Acc: 0.5851\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.1120 Acc: 0.9650\n",
      "test Loss: 0.9949 Acc: 0.7121\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.1684 Acc: 0.9450\n",
      "test Loss: 1.7622 Acc: 0.5759\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.1916 Acc: 0.9450\n",
      "test Loss: 1.1678 Acc: 0.6780\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.1803 Acc: 0.9450\n",
      "test Loss: 0.9727 Acc: 0.7245\n",
      "Best val Acc: 0.758514\n",
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "Evaluation results -> ACC: 0.7585 - LOSS: 0.9142\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts:\")\n",
    "trained_model = train_imageonly_attributor(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_attributor.pth'))\n",
    "eval_imageonly_attributor(test_model, dataloaders, dataset_sizes) #valuta se serve, in fondo il test lo fai durante il training... viene uguale\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid attributor\n",
    "In this section we will build and train a model similar to the model built in section 1.<br>\n",
    "The difference is that instead of taking as input only the image, this model also considers its textual caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test datasets are generated in the same way as in the image-only attributor case.<br>\n",
    "For the dataset directory structure also refer to the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/train/class_real\", \"data/hybrid_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then denerate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/test/class_real\", \"data/hybrid_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs the class predicted for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "epoch: 1/30\n",
      "EPOCH:  1  - MEAN ACCURACY:  tensor(0.2200)  - MEAN LOSS:  tensor(1.3872)\n",
      "epoch: 2/30\n",
      "EPOCH:  2  - MEAN ACCURACY:  tensor(0.3183)  - MEAN LOSS:  tensor(1.3617)\n",
      "epoch: 3/30\n",
      "EPOCH:  3  - MEAN ACCURACY:  tensor(0.4183)  - MEAN LOSS:  tensor(1.3365)\n",
      "epoch: 4/30\n",
      "EPOCH:  4  - MEAN ACCURACY:  tensor(0.5933)  - MEAN LOSS:  tensor(1.3108)\n",
      "epoch: 5/30\n",
      "EPOCH:  5  - MEAN ACCURACY:  tensor(0.6767)  - MEAN LOSS:  tensor(1.2796)\n",
      "epoch: 6/30\n",
      "EPOCH:  6  - MEAN ACCURACY:  tensor(0.7567)  - MEAN LOSS:  tensor(1.2472)\n",
      "epoch: 7/30\n",
      "EPOCH:  7  - MEAN ACCURACY:  tensor(0.7767)  - MEAN LOSS:  tensor(1.2103)\n",
      "epoch: 8/30\n",
      "EPOCH:  8  - MEAN ACCURACY:  tensor(0.7817)  - MEAN LOSS:  tensor(1.1686)\n",
      "epoch: 9/30\n",
      "EPOCH:  9  - MEAN ACCURACY:  tensor(0.8517)  - MEAN LOSS:  tensor(1.1257)\n",
      "epoch: 10/30\n",
      "EPOCH:  10  - MEAN ACCURACY:  tensor(0.7917)  - MEAN LOSS:  tensor(1.0752)\n",
      "epoch: 11/30\n",
      "EPOCH:  11  - MEAN ACCURACY:  tensor(0.8250)  - MEAN LOSS:  tensor(1.0234)\n",
      "epoch: 12/30\n",
      "EPOCH:  12  - MEAN ACCURACY:  tensor(0.8917)  - MEAN LOSS:  tensor(0.9716)\n",
      "epoch: 13/30\n",
      "EPOCH:  13  - MEAN ACCURACY:  tensor(0.8350)  - MEAN LOSS:  tensor(0.9214)\n",
      "epoch: 14/30\n",
      "EPOCH:  14  - MEAN ACCURACY:  tensor(0.8517)  - MEAN LOSS:  tensor(0.8695)\n",
      "epoch: 15/30\n",
      "EPOCH:  15  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.8177)\n",
      "epoch: 16/30\n",
      "EPOCH:  16  - MEAN ACCURACY:  tensor(0.8950)  - MEAN LOSS:  tensor(0.7632)\n",
      "epoch: 17/30\n",
      "EPOCH:  17  - MEAN ACCURACY:  tensor(0.9150)  - MEAN LOSS:  tensor(0.7183)\n",
      "epoch: 18/30\n",
      "EPOCH:  18  - MEAN ACCURACY:  tensor(0.9100)  - MEAN LOSS:  tensor(0.6733)\n",
      "epoch: 19/30\n",
      "EPOCH:  19  - MEAN ACCURACY:  tensor(0.9300)  - MEAN LOSS:  tensor(0.6240)\n",
      "epoch: 20/30\n",
      "EPOCH:  20  - MEAN ACCURACY:  tensor(0.9067)  - MEAN LOSS:  tensor(0.5921)\n",
      "epoch: 21/30\n",
      "EPOCH:  21  - MEAN ACCURACY:  tensor(0.9400)  - MEAN LOSS:  tensor(0.5433)\n",
      "epoch: 22/30\n",
      "EPOCH:  22  - MEAN ACCURACY:  tensor(0.9400)  - MEAN LOSS:  tensor(0.5106)\n",
      "epoch: 23/30\n",
      "EPOCH:  23  - MEAN ACCURACY:  tensor(0.9450)  - MEAN LOSS:  tensor(0.4734)\n",
      "epoch: 24/30\n",
      "EPOCH:  24  - MEAN ACCURACY:  tensor(0.9500)  - MEAN LOSS:  tensor(0.4463)\n",
      "epoch: 25/30\n",
      "EPOCH:  25  - MEAN ACCURACY:  tensor(0.9600)  - MEAN LOSS:  tensor(0.4143)\n",
      "epoch: 26/30\n",
      "EPOCH:  26  - MEAN ACCURACY:  tensor(0.9600)  - MEAN LOSS:  tensor(0.3839)\n",
      "epoch: 27/30\n",
      "EPOCH:  27  - MEAN ACCURACY:  tensor(0.9667)  - MEAN LOSS:  tensor(0.3675)\n",
      "epoch: 28/30\n",
      "EPOCH:  28  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3377)\n",
      "epoch: 29/30\n",
      "EPOCH:  29  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3170)\n",
      "epoch: 30/30\n",
      "EPOCH:  30  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.2977)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4).to(device)\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/train/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/train\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "train_data_loader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Training starts:')\n",
    "train_hybrid_attributor(hybrid_attributor, train_data_loader, 30, 0.005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3. Analysis of the likelihood that different text prompts have to generate authentic images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
