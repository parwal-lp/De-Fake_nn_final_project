{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect and attribute these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "Therefore, this research proposes methods to answer the following 3 research questions [RQ]:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models (See sections 1 and 2)\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to the text-to-image model that generated it (See sections 3 and 4)\n",
    "\n",
    "- **RQ3**. Analysis of the likelihood that different text prompts have to generate authentic images\n",
    "\n",
    "<br>\n",
    "This notebook contains the instructions to reproduce the entire project from scratch, from the creation of the datasets to the design and training of the models.<br><br>\n",
    "\n",
    "For examples on how to use the implemented models see the __[tldrNotebook](tldr_notebook.ipynb)__, where the pre-built datasets and pre-trained weights can be downloaded.<br>\n",
    "\n",
    "For furhter informations the complete code of this project can be found in the source directory of the public GitHub repository __[Source Code](https://github.com/parwal-lp/De-Fake_nn_final_project/src)__.\n",
    "\n",
    "Before proceeding to run the code in this Notebook, please read the instructions contained in the __[Readme](https://github.com/parwal-lp/De-Fake_nn_final_project/tree/main)__ on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reproduce this project from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the path variables to be used globally in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these paths as described below\n",
    "proj_dir = \"/home/parwal/Documents/test/De-Fake_nn_final_project\"    # set here the absolute path to the root of the current project (De-Fake)\n",
    "clip_dir = \"/home/parwal/Documents/GitHub/CLIP\"    # set here the absolute path to the CLIP directory cloned from GitHub\n",
    "ld_dir = \"/home/parwal/Documents/GitHub/latent-diffusion\"   # set here the absolute path to the LD directory cloned from GitHub\n",
    "glide_dir = \"/home/parwal/Documents/GitHub/glide-text2im\"   # set here the absolute path to the GLIDE directory cloned from GitHub\n",
    "\n",
    "SD_api_key = 'sk-6MTDQWuQSLiU3SIc8GEkQrFK7Yjh85JIj0nTfZZKFwircCQQ' # Set here your Stable Diffusion API key (with enough credit to generate at least 400 images, the inital free 25 credits should be sufficient)\n",
    "\n",
    "# Do not change these paths, they are part of the implementation\n",
    "SD_generated_temp_dir = \"data/generated/SD+MSCOCO/\"\n",
    "GLIDE_generated_temp_dir = \"data/generated/GLIDE+MSCOCO/\"\n",
    "LD_generated_temp_dir = ld_dir + \"/outputs/txt2img-samples/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare all the imports needed in this notebook\n",
    "\n",
    "# External libraries imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# References to other files of this project\n",
    "# Functions for the management of data\n",
    "from src.data_collector import fetchImagesFromMSCOCO\n",
    "from src.dataset_generator import SD_generation, LD_generation, GLIDE_generation\n",
    "from src.format_dataset import format_dataset_binaryclass, formatIntoTrainTest, format_dataset_multiclass\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader\n",
    "# Functions for building and training the models\n",
    "from src.imageonly_detector.model import train_imageonly_detector, eval_imageonly_detector\n",
    "from src.imageonly_attributor.model import train_imageonly_attributor, eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, train_hybrid_detector, eval_hybrid_detector\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, train_hybrid_attributor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1. Detection of images generated by text-to-image generation models\n",
    "\n",
    "The study proposes two detector models:\n",
    "\n",
    "1. **Image-only detector**<br>binary classifier that decides whether an input image is fake or real.\n",
    "\n",
    "2. **Hybrid detector**<br>binary classifier that is able to tell if an image is fake or real, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only detector\n",
    "This model is implemented as a two-layer perceptron, to be used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "All the datasets are constitueted by a set of N real images (labeled 1), and a set of N corresponding fake generated images (labeled 0).\n",
    "\n",
    "Training (on a single dataset):\n",
    "- real images fetched from MSCOCO (class 1)\n",
    "- fake images generated by Stable Diffusion (SD) (class 0)\n",
    "\n",
    "Evaluation (on 3 different datasets):\n",
    "- real images always fetched from MSCOCO (class 1)\n",
    "- fake images generated respectively by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (class 0)\n",
    "\n",
    "The data is structured as follows:\n",
    "\n",
    "imageonly_detector_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by LD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fetch the real images together with their captions, for all the datasets described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(10, '/home/parwal/Documents/GitHub/De-Fake_nn_final_project/src/imageonly_detector')\n",
    "\n",
    "#SD+MSCOCO\n",
    "# The dataset generated using SD will be divided into train and test later on, it is temporarely saved in the directory fetched/MSCOCO_for_SD\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/fetched/MSCOCO_for_SD\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# real images and captions needed as input for the LD model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_LD/class_1\", \"data/imageonly_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "# real images and captions needed as input for the GLIDE model, directly saved in the dataset folder with class 1 = real\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_GLIDE/class_1\", \"data/imageonly_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously fetched captions to generate the fake images.<br><br>\n",
    "Notice that for SD we use the APIs, while for LD and GLIDE we used a downloaded local model.<br><br>\n",
    "Also, SD has a very strict protection against inappropriate text prompts, so it might refuse to process some of the prompts, even if they are legit.<br>\n",
    "This exception is handled by the implemented methods, and ignores the unprocessed images labeling them as invalid, those won't be part of the datasets, and even the real counterparts are not included in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/fetched/MSCOCO_for_SD/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "# #LD+MSCOCO --------------------------------------------------------------------------\n",
    "# #use Latent Diffusion model to generate 50 images starting from the captions fetched before\n",
    "# print(\"generating images using LD...\")\n",
    "# LD_generation(\"data/imageonly_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "# print(\"LD images generated successfully!\")\n",
    "\n",
    "# #GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "# #use GLIDE model to generate 50 images starting from the captions fetched before\n",
    "# print(\"generating images using GLIDE...\")\n",
    "# GLIDE_generation(\"data/imageonly_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "# print(\"GLIDE images generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the collected and generated images from their respective folders into the dataset directory structured as described before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mitico1996@libero.it\n",
    "#valeriocantelmo@libero.it\n",
    "#valeriocantelmo@gmail.com\n",
    "\n",
    "#SD+MSCOCO\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/fetched/MSCOCO_for_SD\", SD_generated_temp_dir, \"data/imageonly_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/imageonly_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/imageonly_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images)<br>Each item of this dataset is transformed in order to achieve better performance.<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader obtained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = 'data/imageonly_detector_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts\")\n",
    "trained_model = train_imageonly_detector(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "# Load a model with the trained weights from the previous step, and evaluate it on test data\n",
    "print(\"Evaluation starts\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_detector(test_model, dataloaders, dataset_sizes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid detector\n",
    "For this problem we again implement a two-layer perceptron for binary classification, but in this case it will take as input not only the images but also their captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "\n",
    "The data is first fetched and generated in the exact same way as the dataset for the image-only detector.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- COLLECT REAL IMAGES FROM MSCOCO -------------------- #\n",
    "\n",
    "#SD+MSCOCO\n",
    "print(\"fetching images for SD...\")\n",
    "fetchImagesFromMSCOCO(\"data/fetched/MSCOCO_for_SD\", \"data/hybrid_detector_data\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"fetching images for LD...\")\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_LD/class_1\", \"data/hybrid_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"fetching images for GLIDE...\")\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_GLIDE/class_1\", \"data/hybrid_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- GENERATE FAKE IMAGES USING SD, LD, GLIDE -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"generating images using SD...\")\n",
    "SD_generation(\"data/hybrid_detector_data/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "print(\"SD images generated successfully!\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"generating images using LD...\")\n",
    "LD_generation(\"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "print(\"LD images generated successfully!\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"generating images using GLIDE...\")\n",
    "GLIDE_generation(\"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "print(\"GLIDE images generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FORMAT THE DATA INTO THE STRUCTURE NEEDED FOR TRAINING/TESTING -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "print(\"Building SD dataset...\")\n",
    "formatIntoTrainTest(\"data/fetched/MSCOCO_for_SD/\", SD_generated_temp_dir, \"data/hybrid_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "print(\"Building LD dataset...\")\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/hybrid_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "print(\"Building GLIDE dataset...\")\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/hybrid_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module.<br>A two-layer perceptron that outputs 0 (fake) or 1 (real) for each sample.<br><br>\n",
    "- **Create Dataset and DataLoader** starting from the row data fetched at 2.1 (jpg images and string captions).<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model.<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "print('Building the model...')\n",
    "hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "\n",
    "#Build the dataset\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_detector_data/mscoco_captions.csv\"\n",
    "real_img_dir = \"data/hybrid_detector_data/train/class_1\"\n",
    "fake_img_dir = \"data/hybrid_detector_data/train/class_0\"\n",
    "train_data_loader = get_dataset_loader(captions_file, real_img_dir, fake_img_dir, clip_dir, proj_dir)\n",
    "\n",
    "#Train the model\n",
    "print('Training starts')\n",
    "train_hybrid_detector(hybrid_detector, train_data_loader, 40, 0.0009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it on some test datasets.<br>\n",
    "In particular we will evaluate it on:<br>\n",
    "- Stable Diffusion (SD), dataset generated from the same image-to-text generator used for the train dataset.\n",
    "- GLIDE\n",
    "- Latent Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the weights we trained in the previous code block\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'], clip_dir, proj_dir)\n",
    "    SDloss, SDacc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {SDacc} - Loss: {SDloss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2. Attribution of the fake images to their source model\n",
    "\n",
    "The study proposes two attributor models:\n",
    "\n",
    "1. **Image-only attributor**<br>multi-class classifier that assigns each input image to its source generation model, given the image only.\n",
    "\n",
    "2. **Hybrid attributor**<br>multi-class classifier that assigns each input image to its source generation model, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only attributor\n",
    "\n",
    "In this section we will build and train a model that is able to assign an image to the model that generated it, given only that image.<br><br>\n",
    "The classes that this model will be able to address are the following:\n",
    "- real image -> class 0\n",
    "- fake image generated by SD -> class 1\n",
    "- fake image generated by LD -> class 2\n",
    "- fake image generated by GLIDE -> class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "\n",
    "We generate two datasets, one for training and one for evaluating the model.<br>\n",
    "The steps needed to generate the two datasets are the same:\n",
    "- fetch real images and their captions from MSCOCO (class 0)\n",
    "- generate fake images with SD using the captions of the real images (class 1)\n",
    "- generate fake images with LD using the captions of the real images (class 2)\n",
    "- generate fake images with GLIDE using the captions of the real images (class 3)\n",
    "- move the real and generated images into a dataset directory, with the following structure:\n",
    "\n",
    "imageonly_attributor_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_real/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_SD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by GLIDE*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by LD*<br>\n",
    "&emsp;&emsp;├── test/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sposto le immagini LD\n",
      "sposto le immagini SD\n",
      "['84610.jpg', '331551.jpg', '434986.jpg', '89589.jpg', '165835.jpg', '264884.jpg', '471335.jpg', '391519.jpg', '117466.jpg', '29715.jpg', '564204.jpg', '27157.jpg']\n",
      "sposto le immagini GLIDE\n"
     ]
    }
   ],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/train/class_real\", \"data/imageonly_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sposto le immagini LD\n",
      "sposto le immagini SD\n",
      "['84610.jpg', '331551.jpg', '434986.jpg', '89589.jpg', '516774.jpg', '165835.jpg', '264884.jpg', '471335.jpg', '391519.jpg', '117466.jpg', '29715.jpg', '564204.jpg', '451012.jpg', '27157.jpg']\n",
      "sposto le immagini GLIDE\n"
     ]
    }
   ],
   "source": [
    "# Repeat the same procedure for the test dataset\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/test/class_real\", \"data/imageonly_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images)<br>Each item of this dataset is transformed -- TO DO WHY TRANSFORM CAPIRE --<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 6.4730 Acc: 0.1357\n",
      "test Loss: 4.1937 Acc: 0.3283\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.1459 Acc: 0.6633\n",
      "test Loss: 2.2066 Acc: 0.5404\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.3689 Acc: 0.7487\n",
      "test Loss: 1.1153 Acc: 0.7323\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.3959 Acc: 0.6985\n",
      "test Loss: 1.8747 Acc: 0.6162\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.9474 Acc: 0.7487\n",
      "test Loss: 1.2790 Acc: 0.7323\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.7944 Acc: 0.8141\n",
      "test Loss: 1.3391 Acc: 0.6212\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.5875 Acc: 0.8191\n",
      "test Loss: 1.1329 Acc: 0.6414\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.5855 Acc: 0.8442\n",
      "test Loss: 1.0106 Acc: 0.7222\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.4750 Acc: 0.8693\n",
      "test Loss: 1.2423 Acc: 0.6313\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.3602 Acc: 0.8643\n",
      "test Loss: 1.2142 Acc: 0.6919\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.5698 Acc: 0.7990\n",
      "test Loss: 1.4487 Acc: 0.6818\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.5413 Acc: 0.8442\n",
      "test Loss: 1.0049 Acc: 0.6818\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.3958 Acc: 0.8844\n",
      "test Loss: 1.0912 Acc: 0.6970\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.5242 Acc: 0.8392\n",
      "test Loss: 1.0323 Acc: 0.6566\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.4630 Acc: 0.8442\n",
      "test Loss: 0.9918 Acc: 0.6869\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.4088 Acc: 0.8794\n",
      "test Loss: 1.0560 Acc: 0.6818\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.3422 Acc: 0.8794\n",
      "test Loss: 0.8875 Acc: 0.7677\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.5104 Acc: 0.8392\n",
      "test Loss: 0.7247 Acc: 0.7778\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.3926 Acc: 0.8342\n",
      "test Loss: 0.9933 Acc: 0.6869\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.4035 Acc: 0.8744\n",
      "test Loss: 1.0528 Acc: 0.7374\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.3550 Acc: 0.8894\n",
      "test Loss: 1.0625 Acc: 0.6869\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.3789 Acc: 0.8693\n",
      "test Loss: 1.6836 Acc: 0.6212\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4128 Acc: 0.8894\n",
      "test Loss: 0.8019 Acc: 0.7222\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.2805 Acc: 0.8894\n",
      "test Loss: 0.9073 Acc: 0.7121\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.1783 Acc: 0.9296\n",
      "test Loss: 0.6806 Acc: 0.7879\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.9598\n",
      "test Loss: 0.9834 Acc: 0.7121\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.2174 Acc: 0.9196\n",
      "test Loss: 1.0556 Acc: 0.7121\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.3886 Acc: 0.8693\n",
      "test Loss: 0.7149 Acc: 0.7576\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.2403 Acc: 0.9196\n",
      "test Loss: 1.5844 Acc: 0.6162\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.4080 Acc: 0.8744\n",
      "test Loss: 1.1482 Acc: 0.6919\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.5848 Acc: 0.8543\n",
      "test Loss: 1.0644 Acc: 0.6970\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3186 Acc: 0.8995\n",
      "test Loss: 0.8004 Acc: 0.6919\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.3454 Acc: 0.8744\n",
      "test Loss: 0.7619 Acc: 0.7424\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.3564 Acc: 0.8894\n",
      "test Loss: 0.6990 Acc: 0.7374\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.2462 Acc: 0.9095\n",
      "test Loss: 0.8890 Acc: 0.7121\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 0.9296\n",
      "test Loss: 0.9353 Acc: 0.7020\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.3425 Acc: 0.8995\n",
      "test Loss: 0.6342 Acc: 0.7828\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.3496 Acc: 0.8794\n",
      "test Loss: 0.9680 Acc: 0.7121\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.4418 Acc: 0.8593\n",
      "test Loss: 0.6472 Acc: 0.7980\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2074 Acc: 0.9296\n",
      "test Loss: 0.8659 Acc: 0.7626\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.2653 Acc: 0.9196\n",
      "test Loss: 0.8683 Acc: 0.7121\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.2920 Acc: 0.9095\n",
      "test Loss: 1.1682 Acc: 0.6616\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.2058 Acc: 0.9196\n",
      "test Loss: 0.7168 Acc: 0.7778\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.2879 Acc: 0.8794\n",
      "test Loss: 0.8984 Acc: 0.7475\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.1691 Acc: 0.9397\n",
      "test Loss: 0.8766 Acc: 0.7374\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.1523 Acc: 0.9497\n",
      "test Loss: 0.9282 Acc: 0.7374\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.2119 Acc: 0.9196\n",
      "test Loss: 0.7668 Acc: 0.7525\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.2965 Acc: 0.9095\n",
      "test Loss: 0.8604 Acc: 0.7222\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.2739 Acc: 0.8945\n",
      "test Loss: 1.1760 Acc: 0.6465\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.2559 Acc: 0.9447\n",
      "test Loss: 0.9599 Acc: 0.7374\n",
      "Best val Acc: 0.797980\n",
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "Evaluation results -> ACC: 0.7980 - LOSS: 0.6471\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts:\")\n",
    "trained_model = train_imageonly_attributor(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_attributor.pth'))\n",
    "eval_imageonly_attributor(test_model, dataloaders, dataset_sizes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid attributor\n",
    "In this section we will build and train a model similar to the model built in section 1.<br>\n",
    "The difference is that instead of taking as input only the image, this model also considers its textual caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test datasets are generated in the same way as in the image-only attributor case.<br>\n",
    "For the dataset directory structure also refer to the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "total base parameters 385030726\n",
      "total upsampler parameters 398361286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6633ebc4205492295d36dbfab21535f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fb3d30c51e4b72a403f5bb76ebf434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_83190.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8bdf0f4665492f899cdf214913a6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf458c1a13c4ab28f13c452e8e8e37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_182860.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb95be6ad1ed4b7595818edf63294de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41bcac856fa45e4b7c13db05431a225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_579995.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eada4183a51448b9a519a7e9847f4719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72062c22a07c4dc9b9e271cbe531ad29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_197266.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de2d689ad78410994117040c7015697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c989207d3a5545bbbaa739e7701dce26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_290403.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7327f94965442648f7d4fd6f4c50f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65eaff092da442499c81a2524e31521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_520950.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110bbf83da3642e9bd7ea5035e64cd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b1b3c0e61443c2a683875ef5e39eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_23363.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d451234c3834baabe4b7b425144bb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d377f6c0c4f43278e9ed7430bd812e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_320816.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b46905aa31a45a58e224a3524212753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5b4a6fdc2246719bae3b99a3222ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_169360.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121a88c54997432ea3ebaa577e9bcc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5524ea9822de4ac7ba30f72baf6970e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_278763.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc6f0771ea54b34b526e49977cd03e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79aa5886a14945c5bd2b0ee25ebf6ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_364941.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1c807a7f454c94a06889ea54f1e63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589db8d0853f4304a8d1797e9a725de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_106096.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53eed2f1478a4dd4bf033b9d4402bfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bd4fff69224900ad3fb7a0cdb55c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_348957.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca755475755d44edb839ef69d4964d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2985f96ac34a88867353b92f522978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_59335.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a581b8db744426bb08dbed5522738e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dfde8d8f14449f92f6bb45dff0e989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_244754.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c856d39e1046e392171e0704754f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36e5be983294c689363ffd87478db73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_238980.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6127d447efbd44eb8b17ac2ba3d3dec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ac238371f34cdfadfd32f310cc9f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_117466.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6d3ebd0d644f40b426e630d6323953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6291cc286c4843228280e2b51ae72655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_442324.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce949c9522948b1a6ae50e8d94bcbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c131f982cd8c434b8f1c9c6e5d10695c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_407112.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c789b10b94a40599fcf2900456d0720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225b7a33e38546dc8038b052af04a681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_566274.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decf062274244776b71baa6f9c2f7de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f143797aa699464a9e7dd00a894147bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_509406.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e695ac1e5754645a9ab68526f0ccfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed95f18c630843429736ea8b95ac2c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_536146.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d6afc2ed624f7ba0cb0e14727c25e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877540940d56449aa92d413d616ded58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_376208.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbf457930334304b550df6dd73b066e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8069ce5d77ef48f5b2a023477686fea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_107227.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39222496c6804d888257117e546d3d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1039a2fafb364472b14340d8e261ed2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_268512.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c47e100dde44b88936aa9b4fee84ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bbfffdbdcd495d9e1a58bd572521bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_316917.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f375326ad14ec3824e0d01348917f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e858c2fa2c8466ea5357168f0439cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_430692.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9c578b86384eba9050cde2e59be4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8e12c674e74158bd5ebfc3eb4fa218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_159299.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76653c66e934c349e295262fb6819de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4cd9f78bcc4f0c895090c1d523eb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_346924.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f837f79bf0a4cc5b0496ddb12795431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37815a6ec86f43e39ad04fb33a7a2262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_322008.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1b4a35055c40e79c86402b5535dde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bf2632de6d43bcbfea8e7b436fad59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_439615.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9a86c5ff064033ae83618a0a92831f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2d615fde264cedb17da244f6118fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_458461.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88c893660754f519ed9dee8ed53d7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cb12e59f59467995bfca683aaa83ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_107009.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433b72664f374e7d8b25bc2ee6108e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669aae4dfa5f4bccaa751614a174b8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_280297.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bd487a6d814ae1bfd65cde5fc697d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630095a3b2f04693b9dc1ddb410bd57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_240028.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e083972dca4dc7b9ffa897b74202a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252b353a89a14b81a826c36a8ba9cc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_202652.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9cde977e674df496cb880a55eea0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64023aef8de94eb8b28ab23f2a95b575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_39671.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669f619268474a2393b88d94c278cd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7aa9126c204e0e926dec8f1482f7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_333946.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592634b0f4be4019b0f8ee0e8a27d3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ff31beec00481ea4dd0f346631475f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_398753.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd10e8b2189b4134800fade8bd0dca59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b740a226a214434b09bcb7e68cb7d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_470819.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f730bd58684bcfbcc284940632db14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6721be628b4740fcb2e8f97432bb2a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_206417.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e78c517a1b546a19eb871fd7d61d4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa31c209c5004160a4f13ec757d4f57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_136770.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96576c0377f401ab09eddf339e8c41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe49847fa05a4233bc65ae5e75b23298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_256809.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2230edee3846b4abaad5232ddd8c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a75414391c54ac8806c51d5c9736037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_364176.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e159205d316b41e68792e26438c3fbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6403d7b3204f4942bef0ed7d881399fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_499623.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7626b3c5229f405fae861c7b35c35c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e1879c30f14681bfadb62c7aa26110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_381832.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96dcfa075eb4bb9a42da617142e147a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877933cbb70e411db6767c7b0d73ca50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_570385.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683ca9429da54b17a578b60061e2ad13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af4eb3697af47df9f5b847c31f16894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_343504.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3e0cc55e7449d39d8c4429857fe121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b2335f320d4425b6b3e6b2d97ef9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_101079.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2216517b28554f2a9fa2ed7f4e99f84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af21ed43ac845abac8a5d74dbac45b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_97010.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fc16154d99430aabd06d51bb25a20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27afd54138b84d879536e00363b913b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_img_id.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c256c06baa294356b9af418320efeeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737231e12fd64d43942a4b4d3a5ff7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_247134.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7daaa48184704e18ac9e6097cdf68918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf63758f2ec469289625419705330fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_477585.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdff304cefda44009cd5c720387c198a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7a70d97ad942acbe6e841bd9d5bc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_99724.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c138112522b1457686fb0abb12e17db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad412713b924fc4ac17664340d83360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_418135.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578faf22990a426586d1fc61f021e46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8eea54b02e4947a3b2fe6b653233a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_354572.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d523d1f8fea345e5b9b1835f5abb5590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a322b1ff9ce4973a57a0577e4fab232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_426828.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddc4437f53e4552b73b8b3716a7bf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c725060039470b97dd6ed1de582b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_396608.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2543898a95a54a1aa8171291ad321fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d360cd779948889a5d4a68a553f134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_374201.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00fb83456594054978e474fceebb645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d115746dc8493b95a4f3be1842524e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_245096.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb8791a0d3e48858f43e2b7db016e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf6ea7ee2f1489cb3d602227b83a963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_379108.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f1efd0ecd346c3889cabe1a83cd1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedc34856dde4ff7aa77b0ecffa9fae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_20839.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57387a3876d4a2e809e2bb0811c1650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3db2c024e246ee920147d4beadeaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_246106.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262b8cf5cbaa498487af8a495e1a78eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84961b60f9b4a889346d2e75996cea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_579693.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9b453528b94e89945798e39f169f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1748807a37f4f07af949c60340d5631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_416810.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c96be7a63114f5e9dd34c72167fd45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c51ac483ea6428dac5d2cb1ea624001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_440285.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1841d5ad3bc4472bd466839fd20e4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cba635c2c84ea59bf0449242f42690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_162488.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67b05fe341b41989541ebe9cc7c1cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7519ad34adf743298de95a4fd3197b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved data/generated/GLIDE+MSCOCO//fake_314430.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca50b9c19c7455d8e48ccb47c970950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# fetch the images with their captions from MSCOCO (N=50)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/train/class_real\", \"data/hybrid_attributor_data/train\", 50)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m# # use the same 50 captions to generate images with GLIDE\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m GLIDE_generation(\u001b[39m\"\u001b[39;49m\u001b[39mdata/hybrid_attributor_data/train/mscoco_captions.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, GLIDE_generated_temp_dir, glide_dir)\n\u001b[1;32m     10\u001b[0m \u001b[39m# # use the same 50 captions to generate images with LD OK\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# LD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[39m# # move the generated images to the dataset dir\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/train\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/test/De-Fake_nn_final_project/src/dataset_generator.py:213\u001b[0m, in \u001b[0;36mGLIDE_generation\u001b[0;34m(captions_file, image_save_dir, glide_dir)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m# Sample from the base model.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m model\u001b[39m.\u001b[39mdel_cache()\n\u001b[0;32m--> 213\u001b[0m samples \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39;49mp_sample_loop(\n\u001b[1;32m    214\u001b[0m     model_fn,\n\u001b[1;32m    215\u001b[0m     (full_batch_size, \u001b[39m3\u001b[39;49m, options[\u001b[39m\"\u001b[39;49m\u001b[39mimage_size\u001b[39;49m\u001b[39m\"\u001b[39;49m], options[\u001b[39m\"\u001b[39;49m\u001b[39mimage_size\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[1;32m    216\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    217\u001b[0m     clip_denoised\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    218\u001b[0m     progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    219\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    220\u001b[0m     cond_fn\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    221\u001b[0m )[:batch_size]\n\u001b[1;32m    222\u001b[0m model\u001b[39m.\u001b[39mdel_cache()\n\u001b[1;32m    224\u001b[0m \u001b[39m# ----- UPSCALE IMAGE WITH UPSAMPLER MODEL --------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/gaussian_diffusion.py:389\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39mGenerate samples from the model.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39m:return: a non-differentiable batch of samples.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m final \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp_sample_loop_progressive(\n\u001b[1;32m    390\u001b[0m     model,\n\u001b[1;32m    391\u001b[0m     shape,\n\u001b[1;32m    392\u001b[0m     noise\u001b[39m=\u001b[39mnoise,\n\u001b[1;32m    393\u001b[0m     clip_denoised\u001b[39m=\u001b[39mclip_denoised,\n\u001b[1;32m    394\u001b[0m     denoised_fn\u001b[39m=\u001b[39mdenoised_fn,\n\u001b[1;32m    395\u001b[0m     cond_fn\u001b[39m=\u001b[39mcond_fn,\n\u001b[1;32m    396\u001b[0m     model_kwargs\u001b[39m=\u001b[39mmodel_kwargs,\n\u001b[1;32m    397\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    398\u001b[0m     progress\u001b[39m=\u001b[39mprogress,\n\u001b[1;32m    399\u001b[0m ):\n\u001b[1;32m    400\u001b[0m     final \u001b[39m=\u001b[39m sample\n\u001b[1;32m    401\u001b[0m \u001b[39mreturn\u001b[39;00m final[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/gaussian_diffusion.py:441\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop_progressive\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[1;32m    439\u001b[0m t \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mtensor([i] \u001b[39m*\u001b[39m shape[\u001b[39m0\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    440\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 441\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample(\n\u001b[1;32m    442\u001b[0m         model,\n\u001b[1;32m    443\u001b[0m         img,\n\u001b[1;32m    444\u001b[0m         t,\n\u001b[1;32m    445\u001b[0m         clip_denoised\u001b[39m=\u001b[39;49mclip_denoised,\n\u001b[1;32m    446\u001b[0m         denoised_fn\u001b[39m=\u001b[39;49mdenoised_fn,\n\u001b[1;32m    447\u001b[0m         cond_fn\u001b[39m=\u001b[39;49mcond_fn,\n\u001b[1;32m    448\u001b[0m         model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m     \u001b[39myield\u001b[39;00m out\n\u001b[1;32m    451\u001b[0m     img \u001b[39m=\u001b[39m out[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/gaussian_diffusion.py:340\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_sample\u001b[39m(\n\u001b[1;32m    314\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    315\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m     model_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m ):\n\u001b[1;32m    323\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39m    Sample x_{t-1} from the model at the given timestep.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39m             - 'pred_xstart': a prediction of x_0.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_mean_variance(\n\u001b[1;32m    341\u001b[0m         model,\n\u001b[1;32m    342\u001b[0m         x,\n\u001b[1;32m    343\u001b[0m         t,\n\u001b[1;32m    344\u001b[0m         clip_denoised\u001b[39m=\u001b[39;49mclip_denoised,\n\u001b[1;32m    345\u001b[0m         denoised_fn\u001b[39m=\u001b[39;49mdenoised_fn,\n\u001b[1;32m    346\u001b[0m         model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    348\u001b[0m     noise \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mrandn_like(x)\n\u001b[1;32m    349\u001b[0m     nonzero_mask \u001b[39m=\u001b[39m (\n\u001b[1;32m    350\u001b[0m         (t \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m([\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)))\n\u001b[1;32m    351\u001b[0m     )  \u001b[39m# no noise when t == 0\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/respace.py:94\u001b[0m, in \u001b[0;36mSpacedDiffusion.p_mean_variance\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\u001b[39mself\u001b[39m, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mp_mean_variance(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_model(model), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/gaussian_diffusion.py:234\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m B, C \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m t\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (B,)\n\u001b[0;32m--> 234\u001b[0m model_output \u001b[39m=\u001b[39m model(x, t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model_output, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    236\u001b[0m     model_output, extra \u001b[39m=\u001b[39m model_output\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/respace.py:117\u001b[0m, in \u001b[0;36m_WrappedModel.__call__\u001b[0;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m map_tensor \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestep_map, device\u001b[39m=\u001b[39mts\u001b[39m.\u001b[39mdevice, dtype\u001b[39m=\u001b[39mts\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    116\u001b[0m new_ts \u001b[39m=\u001b[39m map_tensor[ts]\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, new_ts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/test/De-Fake_nn_final_project/src/dataset_generator.py:169\u001b[0m, in \u001b[0;36mGLIDE_generation.<locals>.model_fn\u001b[0;34m(x_t, ts, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m half \u001b[39m=\u001b[39m x_t[: \u001b[39mlen\u001b[39m(x_t) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m    168\u001b[0m combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([half, half], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m model_out \u001b[39m=\u001b[39m model(combined, ts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m eps, rest \u001b[39m=\u001b[39m model_out[:, :\u001b[39m3\u001b[39m], model_out[:, \u001b[39m3\u001b[39m:]\n\u001b[1;32m    171\u001b[0m cond_eps, uncond_eps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(eps, \u001b[39mlen\u001b[39m(eps) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/text2im_model.py:127\u001b[0m, in \u001b[0;36mText2ImUNet.forward\u001b[0;34m(self, x, timesteps, tokens, mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embed(timestep_embedding(timesteps, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_channels))\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxf_width:\n\u001b[0;32m--> 127\u001b[0m     text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_text_emb(tokens, mask)\n\u001b[1;32m    128\u001b[0m     xf_proj, xf_out \u001b[39m=\u001b[39m text_outputs[\u001b[39m\"\u001b[39m\u001b[39mxf_proj\u001b[39m\u001b[39m\"\u001b[39m], text_outputs[\u001b[39m\"\u001b[39m\u001b[39mxf_out\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    129\u001b[0m     emb \u001b[39m=\u001b[39m emb \u001b[39m+\u001b[39m xf_proj\u001b[39m.\u001b[39mto(emb)\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/text2im_model.py:103\u001b[0m, in \u001b[0;36mText2ImUNet.get_text_emb\u001b[0;34m(self, tokens, mask)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39massert\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     xf_in \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mwhere(mask[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m], xf_in, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_embedding[\u001b[39mNone\u001b[39;00m])\n\u001b[0;32m--> 103\u001b[0m xf_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(xf_in\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype))\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_ln \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     xf_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_ln(xf_out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/xf.py:129\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: th\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    128\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblocks:\n\u001b[0;32m--> 129\u001b[0m         x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/xf.py:99\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: th\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 99\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x))\n\u001b[1;32m    100\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x))\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/xf.py:43\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     42\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_qkv(x)\n\u001b[0;32m---> 43\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(x)\n\u001b[1;32m     44\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(x)\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/glide-text2im/glide_text2im/xf.py:77\u001b[0m, in \u001b[0;36mQKVMultiheadAttention.forward\u001b[0;34m(self, qkv)\u001b[0m\n\u001b[1;32m     75\u001b[0m wdtype \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mdtype\n\u001b[1;32m     76\u001b[0m weight \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39msoftmax(weight\u001b[39m.\u001b[39mfloat(), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype(wdtype)\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m th\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mbhts,bshc->bthc\u001b[39;49m\u001b[39m\"\u001b[39;49m, weight, v)\u001b[39m.\u001b[39mreshape(bs, n_ctx, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RIFARE NON SO PER QUALE MOTIVO HO FETCHATO DUE VOLTE, ORA HO 100 FOTO, CAMBIARE ANCHE API KEY DI SD\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "# fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/train/class_real\", \"data/hybrid_attributor_data/train\", 50)\n",
    "\n",
    "# # use the same 50 captions to generate images with SD\n",
    "# SD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir, SD_api_key)\n",
    "\n",
    "# # use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir, glide_dir)\n",
    "\n",
    "# # use the same 50 captions to generate images with LD OK\n",
    "# LD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", ld_dir, proj_dir)\n",
    "\n",
    "# # move the generated images to the dataset dir\n",
    "# format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then denerate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/test/class_real\", \"data/hybrid_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs the class predicted for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4).to(device)\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/train/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/train\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "train_data_loader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Training starts:')\n",
    "train_hybrid_attributor(hybrid_attributor, train_data_loader, 30, 0.005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3. Analysis of the likelihood that different text prompts have to generate authentic images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
