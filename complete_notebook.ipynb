{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Final Project\n",
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect and attribute these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "Therefore, this research proposes methods to answer the following 3 research questions [RQ]:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to their source model\n",
    "\n",
    "- **RQ3**. Analysis of the likelihood that different text prompts have to generate authentic images\n",
    "\n",
    "<br><br>\n",
    "The following sections contain instructions on how to build, train and evaluate models to answer the proposed researched questions.<br><br>\n",
    "The complete code of this project can be found in the source directory of the GitHub repository __[Source Code](https://github.com/parwal-lp/De-Fake_nn_final_project)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare the variables to be used globally in this notebook\n",
    "\n",
    "path_to_ld = \"../latent-diffusion/\" # set here the path to the LD directory cloned from GitHub\n",
    "proj_dir = \"../De-Fake_nn_final_project\" # set here the path to the root of the current project (De-Fake)\n",
    "\n",
    "SD_generated_temp_dir = \"data/generated/SD+MSCOCO/\"\n",
    "GLIDE_generated_temp_dir = \"data/generated/GLIDE+MSCOCO/\"\n",
    "LD_generated_temp_dir = path_to_ld + \"outputs/txt2img-samples/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare all the imports needed in this notebook\n",
    "\n",
    "# External libraries imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# References to other files of this project\n",
    "# Functions for the management of data\n",
    "from src.data_collector import fetchImagesFromMSCOCO\n",
    "from src.dataset_generator import SD_generation, LD_generation, GLIDE_generation\n",
    "from src.format_dataset import format_dataset_binaryclass, formatIntoTrainTest, format_dataset_multiclass\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader\n",
    "# Functions for building and training the models\n",
    "from src.imageonly_detector.model import train_imageonly_detector, eval_imageonly_detector\n",
    "from src.imageonly_attributor.model import train_imageonly_attributor, eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, train_hybrid_detector, eval_hybrid_detector\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, train_hybrid_attributor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1. Detection of images generated by text-to-image generation models\n",
    "\n",
    "The study proposes two detector models:\n",
    "\n",
    "1. **Image-only detector**<br>binary classifier that decides whether an input image is fake or real.\n",
    "\n",
    "2. **Hybrid detector**<br>binary classifier that is able to tell if an image is fake or real, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only detector\n",
    "This model is implemented as a two-layer perceptron, to be used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "All the datasets are constitueted by a set of N real images (labeled 1), and a set of N corresponding fake generated images (labeled 0).\n",
    "\n",
    "Training (on a single dataset):\n",
    "- real images fetched from MSCOCO (class 1)\n",
    "- fake images generated by Stable Diffusion (SD) (class 0)\n",
    "\n",
    "Evaluation (on three different datasets):\n",
    "- real images always fetched from MSCOCO (class 1)\n",
    "- fake images generated respectively by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (class 0)\n",
    "\n",
    "The data is structured as follows:\n",
    "\n",
    "imageonly_detector_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by LD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the path to the scripts needed for this section\n",
    "sys.path.insert(10, '/home/parwal/Documents/GitHub/De-Fake_nn_final_project/src/imageonly_detector')\n",
    "#TODO capire a chi serve questo import e metterlo nel posto giusto\n",
    "\n",
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO\n",
    "# The dataset generated using SD will be divided into train and test later on\n",
    "fetchImagesFromMSCOCO(\"data/MSCOCO_for_SD/images\", \"data/MSCOCO_for_SD\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# real images and captions needed as input for the LD model\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_LD/class_1\", \"data/imageonly_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "# real images and captions needed as input for the GLIDE model\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_GLIDE/class_1\", \"data/imageonly_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "#prima di eseguire il file ho cambiato le directory\n",
    "#%run src/imageonly_detector/SD_MSCOCO_data_generation.py\n",
    "SD_generation(\"data/MSCOCO_for_SD/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "#resetto la directory corrente a quella del progetto de-fake, altrimenti il file da eseguire non viene trovato\n",
    "#questo è necessario perché LD_MSCOCO_data_generation.py cambia la directory a quella di latent-diffusion\n",
    "#os.chdir(\"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\")\n",
    "#%run src/imageonly_detector/LD_MSCOCO_data_generation.py\n",
    "LD_generation(\"data/imageonly_detector_data/val_LD/mscoco_captions.csv\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#%run src/imageonly_detector/GLIDE_MSCOCO_data_generation.ipynb\n",
    "GLIDE_generation(\"data/imageonly_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the collected data in the previously described structure\n",
    "\n",
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/MSCOCO_for_SD/images\", SD_generated_temp_dir, \"data/imageonly_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/imageonly_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/imageonly_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images)<br>Each item of this dataset is transformed -- TO DO WHY TRANSFORM CAPIRE --<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 3.1478 Acc: 0.5100\n",
      "val Loss: 0.4255 Acc: 0.8265\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.5736 Acc: 0.8400\n",
      "val Loss: 0.2364 Acc: 0.9388\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.6277 Acc: 0.8300\n",
      "val Loss: 0.4284 Acc: 0.8571\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.5500 Acc: 0.8400\n",
      "val Loss: 0.5303 Acc: 0.8469\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.7450 Acc: 0.8100\n",
      "val Loss: 1.0299 Acc: 0.7143\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.6322 Acc: 0.8100\n",
      "val Loss: 0.3712 Acc: 0.8673\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.2620 Acc: 0.8900\n",
      "val Loss: 0.6136 Acc: 0.8469\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.5283 Acc: 0.8300\n",
      "val Loss: 0.3662 Acc: 0.8571\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.4382 Acc: 0.8400\n",
      "val Loss: 0.3725 Acc: 0.8571\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.2583 Acc: 0.9100\n",
      "val Loss: 0.3593 Acc: 0.8776\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.4544 Acc: 0.8700\n",
      "val Loss: 0.3310 Acc: 0.8265\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.3764 Acc: 0.8700\n",
      "val Loss: 0.2749 Acc: 0.8673\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.2885 Acc: 0.8900\n",
      "val Loss: 0.3027 Acc: 0.8878\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.1382 Acc: 0.9500\n",
      "val Loss: 0.2945 Acc: 0.8776\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.3013 Acc: 0.8900\n",
      "val Loss: 0.2815 Acc: 0.8980\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.3444 Acc: 0.8700\n",
      "val Loss: 0.2924 Acc: 0.8673\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.3577 Acc: 0.8500\n",
      "val Loss: 0.2903 Acc: 0.8673\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.2179 Acc: 0.9300\n",
      "val Loss: 0.3183 Acc: 0.8673\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.2132 Acc: 0.9300\n",
      "val Loss: 0.3012 Acc: 0.8571\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.2730 Acc: 0.8800\n",
      "val Loss: 0.2471 Acc: 0.8776\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.1143 Acc: 0.9700\n",
      "val Loss: 0.2833 Acc: 0.8571\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.3795 Acc: 0.8800\n",
      "val Loss: 0.2984 Acc: 0.8776\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4218 Acc: 0.8400\n",
      "val Loss: 0.2528 Acc: 0.8776\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0951 Acc: 0.9800\n",
      "val Loss: 0.2776 Acc: 0.8776\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.2388 Acc: 0.8900\n",
      "val Loss: 0.2753 Acc: 0.8776\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.2440 Acc: 0.8900\n",
      "val Loss: 0.2767 Acc: 0.8776\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.4718 Acc: 0.8200\n",
      "val Loss: 0.2823 Acc: 0.8776\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.3393 Acc: 0.8900\n",
      "val Loss: 0.2696 Acc: 0.8776\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.3162 Acc: 0.9200\n",
      "val Loss: 0.2740 Acc: 0.8776\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.2343 Acc: 0.8900\n",
      "val Loss: 0.2806 Acc: 0.8776\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.2457 Acc: 0.9000\n",
      "val Loss: 0.3151 Acc: 0.8878\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3116 Acc: 0.8900\n",
      "val Loss: 0.2711 Acc: 0.8776\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.2606 Acc: 0.9300\n",
      "val Loss: 0.2856 Acc: 0.8878\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.2339 Acc: 0.9000\n",
      "val Loss: 0.2874 Acc: 0.8776\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.3152 Acc: 0.8800\n",
      "val Loss: 0.2676 Acc: 0.8878\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.3629 Acc: 0.8500\n",
      "val Loss: 0.2823 Acc: 0.8776\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.1239 Acc: 0.9300\n",
      "val Loss: 0.2778 Acc: 0.8673\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.2192 Acc: 0.8800\n",
      "val Loss: 0.3060 Acc: 0.8776\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.2472 Acc: 0.8900\n",
      "val Loss: 0.3023 Acc: 0.8776\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.9600\n",
      "val Loss: 0.3184 Acc: 0.8776\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.3029 Acc: 0.8700\n",
      "val Loss: 0.3159 Acc: 0.8673\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.2238 Acc: 0.8900\n",
      "val Loss: 0.2747 Acc: 0.8776\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.1823 Acc: 0.9100\n",
      "val Loss: 0.2980 Acc: 0.8776\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.3327 Acc: 0.9000\n",
      "val Loss: 0.2975 Acc: 0.8776\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.2853 Acc: 0.9000\n",
      "val Loss: 0.2738 Acc: 0.8878\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.1707 Acc: 0.9200\n",
      "val Loss: 0.2839 Acc: 0.8776\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.2591 Acc: 0.9100\n",
      "val Loss: 0.2679 Acc: 0.8980\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.1952 Acc: 0.9200\n",
      "val Loss: 0.2745 Acc: 0.8776\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.3737 Acc: 0.8500\n",
      "val Loss: 0.3033 Acc: 0.8878\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.1844 Acc: 0.9400\n",
      "val Loss: 0.2911 Acc: 0.8776\n",
      "\n",
      "Training complete in 0m 47s\n",
      "Best val Acc: 0.938776\n",
      "Evaluation starts\n",
      "loading model with trained weights...\n",
      "val_LD Loss: 1.4511 Acc: 0.6800\n",
      "val_GLIDE Loss: 1.1783 Acc: 0.7000\n",
      "Evaluation complete in 0m 1s\n"
     ]
    }
   ],
   "source": [
    " # Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_LD': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val_GLIDE': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_detector_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'val_LD', 'val_GLIDE']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts\")\n",
    "trained_model = train_imageonly_detector(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_detector(test_model, dataloaders, dataset_sizes) #valuta se serve, in fondo il test lo fai durante il training... viene uguale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid detector\n",
    "For this problem we again implement a two-layer perceptron for binary classification, but in this case it will take as input not only the images but also their captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "\n",
    "The data is first fetched and generated in the exact same way as the dataset for the image-only detector.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- COLLECT REAL IMAGES FROM MSCOCO -------------------- #\n",
    "\n",
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO\n",
    "fetchImagesFromMSCOCO(\"data/MSCOCO_for_SD_hybrid/images\", \"data/MSCOCO_for_SD_hybrid\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_LD/class_1\", \"data/hybrid_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_GLIDE/class_1\", \"data/hybrid_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- GENERATE FAKE IMAGES USING SD, LD, GLIDE -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "#%run src/imageonly_detector/SD_MSCOCO_data_generation.py\n",
    "SD_generation(\"data/MSCOCO_for_SD_hybrid/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# N.B.\n",
    "# prima di lanciare questo comando, aggiungere il file src/imageonly_detector/txt2img_batch.py alla directory latent-diffusion/scripts/\n",
    "#resetto la directory corrente a quella del progetto de-fake, altrimenti il file da eseguire non viene trovato\n",
    "#questo è necessario perché LD_MSCOCO_data_generation.py cambia la directory a quella di latent-diffusion\n",
    "#os.chdir(\"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\")\n",
    "#%run src/imageonly_detector/LD_MSCOCO_data_generation_batch.py\n",
    "LD_generation(\"data/hybrid_detector_data/val_LD/mscoco_captions.csv\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#NON HO MAI PROVATO A RUNNARLO, altrimenti rigenera il modello (3gb)\n",
    "#provare a runnarlo proprio alla fine di tutto per sicurezza\n",
    "#%run src/imageonly_detector/GLIDE_MSCOCO_data_generation.ipynb #TODO\n",
    "GLIDE_generation(\"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FORMAT THE DATA INTO THE STRUCTURE NEEDED FOR TRAINING/TESTING -------------------- #\n",
    "os.chdir(\"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\")\n",
    "\n",
    "#transform the collected data in the previously described structure\n",
    "\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/MSCOCO_for_SD_hybrid/images\", SD_generated_temp_dir, \"data/hybrid_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/hybrid_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/hybrid_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for binary classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs 0 (fake) or 1 (real) for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "EPOCH:  1/10  - MEAN ACCURACY:  tensor(0.5000)  - MEAN LOSS:  tensor(0.6976)\n",
      "EPOCH:  2/10  - MEAN ACCURACY:  tensor(0.5400)  - MEAN LOSS:  tensor(0.6853)\n",
      "EPOCH:  3/10  - MEAN ACCURACY:  tensor(0.5900)  - MEAN LOSS:  tensor(0.6740)\n",
      "EPOCH:  4/10  - MEAN ACCURACY:  tensor(0.6200)  - MEAN LOSS:  tensor(0.6629)\n",
      "EPOCH:  5/10  - MEAN ACCURACY:  tensor(0.7800)  - MEAN LOSS:  tensor(0.6523)\n",
      "EPOCH:  6/10  - MEAN ACCURACY:  tensor(0.8200)  - MEAN LOSS:  tensor(0.6393)\n",
      "EPOCH:  7/10  - MEAN ACCURACY:  tensor(0.8700)  - MEAN LOSS:  tensor(0.6267)\n",
      "EPOCH:  8/10  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.6138)\n",
      "EPOCH:  9/10  - MEAN ACCURACY:  tensor(0.8900)  - MEAN LOSS:  tensor(0.5987)\n",
      "EPOCH:  10/10  - MEAN ACCURACY:  tensor(0.9300)  - MEAN LOSS:  tensor(0.5838)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Build the model\n",
    "print('Building the model...')\n",
    "hybrid_detector = TwoLayerPerceptron(1024, 100, 1).to(device)\n",
    "\n",
    "#Build the dataset\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_detector_data/mscoco_captions.csv\"\n",
    "real_img_dir = \"data/hybrid_detector_data/train/class_1\"\n",
    "fake_img_dir = \"data/hybrid_detector_data/train/class_0\"\n",
    "train_data_loader = get_dataset_loader(captions_file, real_img_dir, fake_img_dir)\n",
    "\n",
    "#Train the model\n",
    "print('Training starts')\n",
    "train_hybrid_detector(hybrid_detector, train_data_loader, 10, 0.005) # few epochs are needed because we use transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it on some test datasets.<br>\n",
    "In particular we will evaluate it on:<br>\n",
    "- Stable Diffusion (SD), dataset generated from the same image-to-text generator used for the train dataset.\n",
    "- GLIDE\n",
    "- Latent Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "testing...\n",
      "Evaluation on SD --> Accuracy: 0.8933333158493042 - Loss: 0.5959395170211792\n",
      "Evaluation on GLIDE --> Accuracy: 0.7100000381469727 - Loss: 0.6429540514945984\n",
      "Evaluation on LD --> Accuracy: 0.7100000977516174 - Loss: 0.6497437357902527\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the weights we trained in the previous code block\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'])\n",
    "    SDloss, SDacc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {SDacc} - Loss: {SDloss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2. Attribution of the fake images to their source model\n",
    "\n",
    "The study proposes two attributor models:\n",
    "\n",
    "1. **Image-only attributor**<br>multi-class classifier that assigns each input image to its source generation model, given the image only.\n",
    "\n",
    "2. **Hybrid attributor**<br>multi-class classifier that assigns each input image to its source generation model, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only attributor\n",
    "\n",
    "In this section we will build and train a model that is able to assign an image to the model that generated it, given only that image.<br><br>\n",
    "The classes that this model will be able to address are the following:\n",
    "- real image -> class 0\n",
    "- fake image generated by SD -> class 1\n",
    "- fake image generated by LD -> class 2\n",
    "- fake image generated by GLIDE -> class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "\n",
    "We generate two datasets, one for training and one for evaluating the model.<br>\n",
    "The steps needed to generate the two datasets are the same:\n",
    "- fetch real images and their captions from MSCOCO (class 0)\n",
    "- generate fake images with SD using the captions of the real images (class 1)\n",
    "- generate fake images with LD using the captions of the real images (class 2)\n",
    "- generate fake images with GLIDE using the captions of the real images (class 3)\n",
    "- move the real and generated images into a dataset directory, with the following structure:\n",
    "\n",
    "imageonly_attributor_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_real/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_SD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by GLIDE*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by LD*<br>\n",
    "&emsp;&emsp;├── test/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(proj_dir)\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/train/class_real\", \"data/imageonly_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same procedure for the test dataset\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/test/class_real\", \"data/imageonly_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** starting from a pre-trained version of ResNet18<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images)<br>Each item of this dataset is transformed -- TO DO WHY TRANSFORM CAPIRE --<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 6.2529 Acc: 0.1900\n",
      "test Loss: 6.1089 Acc: 0.3003\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.4839 Acc: 0.6250\n",
      "test Loss: 2.8051 Acc: 0.5387\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.4246 Acc: 0.7500\n",
      "test Loss: 2.4543 Acc: 0.6409\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.8603 Acc: 0.7950\n",
      "test Loss: 2.5517 Acc: 0.5820\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.0108 Acc: 0.7550\n",
      "test Loss: 1.8932 Acc: 0.6285\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.7282 Acc: 0.8350\n",
      "test Loss: 1.6203 Acc: 0.6502\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.6359 Acc: 0.8350\n",
      "test Loss: 1.5583 Acc: 0.6533\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.6271 Acc: 0.7900\n",
      "test Loss: 1.2853 Acc: 0.6997\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.6136 Acc: 0.8250\n",
      "test Loss: 1.0811 Acc: 0.7276\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.5413 Acc: 0.8500\n",
      "test Loss: 0.8983 Acc: 0.7461\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.6380 Acc: 0.7850\n",
      "test Loss: 0.9645 Acc: 0.7399\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.5081 Acc: 0.8400\n",
      "test Loss: 1.8814 Acc: 0.6378\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.5384 Acc: 0.8200\n",
      "test Loss: 1.1042 Acc: 0.7307\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.4571 Acc: 0.8250\n",
      "test Loss: 1.3412 Acc: 0.6718\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.4967 Acc: 0.8400\n",
      "test Loss: 1.1187 Acc: 0.6842\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.5038 Acc: 0.8550\n",
      "test Loss: 1.1006 Acc: 0.6842\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.3284 Acc: 0.8800\n",
      "test Loss: 0.8795 Acc: 0.7461\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.3617 Acc: 0.8700\n",
      "test Loss: 1.2994 Acc: 0.6409\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.2732 Acc: 0.9250\n",
      "test Loss: 1.0355 Acc: 0.7183\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.3832 Acc: 0.8750\n",
      "test Loss: 0.9005 Acc: 0.7430\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.5227 Acc: 0.7900\n",
      "test Loss: 1.5337 Acc: 0.6440\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.2909 Acc: 0.9150\n",
      "test Loss: 1.4403 Acc: 0.6471\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.3293 Acc: 0.8600\n",
      "test Loss: 1.6725 Acc: 0.6161\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.3935 Acc: 0.8900\n",
      "test Loss: 1.0344 Acc: 0.6935\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.4146 Acc: 0.8750\n",
      "test Loss: 0.9368 Acc: 0.7028\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.3399 Acc: 0.8950\n",
      "test Loss: 1.3060 Acc: 0.6749\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.3353 Acc: 0.8900\n",
      "test Loss: 1.0463 Acc: 0.7059\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.2018 Acc: 0.9350\n",
      "test Loss: 0.9865 Acc: 0.6935\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.3950 Acc: 0.8850\n",
      "test Loss: 1.6244 Acc: 0.6780\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.2222 Acc: 0.9250\n",
      "test Loss: 1.1376 Acc: 0.6718\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.3051 Acc: 0.9150\n",
      "test Loss: 1.2627 Acc: 0.6718\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3659 Acc: 0.8850\n",
      "test Loss: 0.9177 Acc: 0.7090\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.3911 Acc: 0.8900\n",
      "test Loss: 1.1161 Acc: 0.6997\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.4674 Acc: 0.8500\n",
      "test Loss: 1.0975 Acc: 0.6811\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.3702 Acc: 0.8750\n",
      "test Loss: 0.9993 Acc: 0.6780\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.2114 Acc: 0.9300\n",
      "test Loss: 1.0395 Acc: 0.6594\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.2256 Acc: 0.9200\n",
      "test Loss: 1.0676 Acc: 0.7059\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.3959 Acc: 0.8900\n",
      "test Loss: 1.1090 Acc: 0.6873\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.1793 Acc: 0.9250\n",
      "test Loss: 1.0671 Acc: 0.6533\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2726 Acc: 0.9100\n",
      "test Loss: 1.3530 Acc: 0.6842\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.3136 Acc: 0.8900\n",
      "test Loss: 1.2996 Acc: 0.6378\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.2802 Acc: 0.8950\n",
      "test Loss: 1.0228 Acc: 0.6811\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.3662 Acc: 0.9000\n",
      "test Loss: 1.3051 Acc: 0.6006\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.3985 Acc: 0.9000\n",
      "test Loss: 1.6337 Acc: 0.6223\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.3261 Acc: 0.9250\n",
      "test Loss: 1.2169 Acc: 0.6409\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.3503 Acc: 0.8700\n",
      "test Loss: 1.5930 Acc: 0.5728\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.2487 Acc: 0.9450\n",
      "test Loss: 1.3082 Acc: 0.6378\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.1893 Acc: 0.9400\n",
      "test Loss: 1.2898 Acc: 0.6378\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9350\n",
      "test Loss: 1.2613 Acc: 0.6842\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.2794 Acc: 0.9200\n",
      "test Loss: 1.2576 Acc: 0.6285\n",
      "Best val Acc: 0.746130\n",
      "Evaluation starts:\n",
      "loading model with trained weights...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained_models/imageonly_detector.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloading model with trained weights...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m test_model \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mresnet18(weights\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIMAGENET1K_V1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m test_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mtrained_models/imageonly_detector.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     36\u001b[0m eval_imageonly_attributor(test_model, dataloaders, dataset_sizes) \u001b[39m#valuta se serve, in fondo il test lo fai durante il training... viene uguale\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_models/imageonly_detector.pth'"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts:\")\n",
    "trained_model = train_imageonly_attributor(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "print(\"loading model with trained weights...\")\n",
    "test_model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "test_model.load_state_dict(torch.load('trained_models/imageonly_detector.pth'))\n",
    "eval_imageonly_attributor(test_model, dataloaders, dataset_sizes) #valuta se serve, in fondo il test lo fai durante il training... viene uguale\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid attributor\n",
    "In this section we will build and train a model similar to the model built in section 1.<br>\n",
    "The difference is that instead of taking as input only the image, this model also considers its textual caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test datasets are generated in the same way as in the image-only attributor case.<br>\n",
    "For the dataset directory structure also refer to the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/train/class_real\", \"data/hybrid_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then denerate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/test/class_real\", \"data/hybrid_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs the class predicted for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "epoch: 1/30\n",
      "EPOCH:  1  - MEAN ACCURACY:  tensor(0.2200)  - MEAN LOSS:  tensor(1.3872)\n",
      "epoch: 2/30\n",
      "EPOCH:  2  - MEAN ACCURACY:  tensor(0.3183)  - MEAN LOSS:  tensor(1.3617)\n",
      "epoch: 3/30\n",
      "EPOCH:  3  - MEAN ACCURACY:  tensor(0.4183)  - MEAN LOSS:  tensor(1.3365)\n",
      "epoch: 4/30\n",
      "EPOCH:  4  - MEAN ACCURACY:  tensor(0.5933)  - MEAN LOSS:  tensor(1.3108)\n",
      "epoch: 5/30\n",
      "EPOCH:  5  - MEAN ACCURACY:  tensor(0.6767)  - MEAN LOSS:  tensor(1.2796)\n",
      "epoch: 6/30\n",
      "EPOCH:  6  - MEAN ACCURACY:  tensor(0.7567)  - MEAN LOSS:  tensor(1.2472)\n",
      "epoch: 7/30\n",
      "EPOCH:  7  - MEAN ACCURACY:  tensor(0.7767)  - MEAN LOSS:  tensor(1.2103)\n",
      "epoch: 8/30\n",
      "EPOCH:  8  - MEAN ACCURACY:  tensor(0.7817)  - MEAN LOSS:  tensor(1.1686)\n",
      "epoch: 9/30\n",
      "EPOCH:  9  - MEAN ACCURACY:  tensor(0.8517)  - MEAN LOSS:  tensor(1.1257)\n",
      "epoch: 10/30\n",
      "EPOCH:  10  - MEAN ACCURACY:  tensor(0.7917)  - MEAN LOSS:  tensor(1.0752)\n",
      "epoch: 11/30\n",
      "EPOCH:  11  - MEAN ACCURACY:  tensor(0.8250)  - MEAN LOSS:  tensor(1.0234)\n",
      "epoch: 12/30\n",
      "EPOCH:  12  - MEAN ACCURACY:  tensor(0.8917)  - MEAN LOSS:  tensor(0.9716)\n",
      "epoch: 13/30\n",
      "EPOCH:  13  - MEAN ACCURACY:  tensor(0.8350)  - MEAN LOSS:  tensor(0.9214)\n",
      "epoch: 14/30\n",
      "EPOCH:  14  - MEAN ACCURACY:  tensor(0.8517)  - MEAN LOSS:  tensor(0.8695)\n",
      "epoch: 15/30\n",
      "EPOCH:  15  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.8177)\n",
      "epoch: 16/30\n",
      "EPOCH:  16  - MEAN ACCURACY:  tensor(0.8950)  - MEAN LOSS:  tensor(0.7632)\n",
      "epoch: 17/30\n",
      "EPOCH:  17  - MEAN ACCURACY:  tensor(0.9150)  - MEAN LOSS:  tensor(0.7183)\n",
      "epoch: 18/30\n",
      "EPOCH:  18  - MEAN ACCURACY:  tensor(0.9100)  - MEAN LOSS:  tensor(0.6733)\n",
      "epoch: 19/30\n",
      "EPOCH:  19  - MEAN ACCURACY:  tensor(0.9300)  - MEAN LOSS:  tensor(0.6240)\n",
      "epoch: 20/30\n",
      "EPOCH:  20  - MEAN ACCURACY:  tensor(0.9067)  - MEAN LOSS:  tensor(0.5921)\n",
      "epoch: 21/30\n",
      "EPOCH:  21  - MEAN ACCURACY:  tensor(0.9400)  - MEAN LOSS:  tensor(0.5433)\n",
      "epoch: 22/30\n",
      "EPOCH:  22  - MEAN ACCURACY:  tensor(0.9400)  - MEAN LOSS:  tensor(0.5106)\n",
      "epoch: 23/30\n",
      "EPOCH:  23  - MEAN ACCURACY:  tensor(0.9450)  - MEAN LOSS:  tensor(0.4734)\n",
      "epoch: 24/30\n",
      "EPOCH:  24  - MEAN ACCURACY:  tensor(0.9500)  - MEAN LOSS:  tensor(0.4463)\n",
      "epoch: 25/30\n",
      "EPOCH:  25  - MEAN ACCURACY:  tensor(0.9600)  - MEAN LOSS:  tensor(0.4143)\n",
      "epoch: 26/30\n",
      "EPOCH:  26  - MEAN ACCURACY:  tensor(0.9600)  - MEAN LOSS:  tensor(0.3839)\n",
      "epoch: 27/30\n",
      "EPOCH:  27  - MEAN ACCURACY:  tensor(0.9667)  - MEAN LOSS:  tensor(0.3675)\n",
      "epoch: 28/30\n",
      "EPOCH:  28  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3377)\n",
      "epoch: 29/30\n",
      "EPOCH:  29  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3170)\n",
      "epoch: 30/30\n",
      "EPOCH:  30  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.2977)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4).to(device)\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/train/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/train\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "train_data_loader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Training starts:')\n",
    "train_hybrid_attributor(hybrid_attributor, train_data_loader, 30, 0.005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3. Analysis of the likelihood that different text prompts have to generate authentic images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
