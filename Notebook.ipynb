{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Final Project\n",
    "### Reimplementation of the study: <br> ***\"DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image GenerationModels\"* <br> from Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang**\n",
    "\n",
    "**Name**: *Laura Papi*\n",
    "\n",
    "**Matricola**: *1760732*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "The above cited study focuses on the growing concerns about the possible misuse of AI generated images, and assesses the necessity for a tool to detect, and attribute, these fake images.<br>\n",
    "In particular, it points out the lack of research on the particular case of images generated by a text prompt.\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "Therefore, this research proposes methods to answer the following 3 research questions [RQ]:\n",
    "\n",
    "- **RQ1**. Detection of images generated by text-to-image generation models\n",
    "\n",
    "- **RQ2**. Attribution of the fake images to their source model\n",
    "\n",
    "- **RQ3**. Analysis of the likelihood that different text prompts have to generate authentic images\n",
    "\n",
    "<br><br>\n",
    "The following sections contain examples for my implementation of the described methods.<br><br>\n",
    "The complete implementation of the models can be found in the source directory of the GitHub repository __[Source Code](http://url)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare the variables to be used globally in this notebook\n",
    "\n",
    "path_to_ld = \"../latent-diffusion/\" # set here the path to the LD directory cloned from GitHub\n",
    "proj_dir = \"../De-Fake_nn_final_project\" # set here the path to the root of the current project (De-Fake)\n",
    "\n",
    "SD_generated_temp_dir = \"data/generated/SD+MSCOCO/\"\n",
    "GLIDE_generated_temp_dir = \"data/generated/GLIDE+MSCOCO/\"\n",
    "LD_generated_temp_dir = path_to_ld + \"outputs/txt2img-samples/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Declare all the imports needed in this notebook\n",
    "\n",
    "# External libraries imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# References to other files of this project\n",
    "from src.data_collector import fetchImagesFromMSCOCO\n",
    "from src.dataset_generator import SD_generation, LD_generation, GLIDE_generation\n",
    "from src.format_dataset import format_dataset_binaryclass, formatIntoTrainTest, format_dataset_multiclass\n",
    "from src.encoder import get_multiclass_dataset_loader, get_dataset_loader\n",
    "\n",
    "from src.hybrid_attributor.model import MultiClassTwoLayerPerceptron, train_hybrid_attributor\n",
    "from src.imageonly_attributor.model import train_imageonly_attributor, eval_imageonly_attributor\n",
    "from src.hybrid_detector.hybrid_detector import TwoLayerPerceptron, train_hybrid_detector, eval_hybrid_detector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1. Detection of images generated by text-to-image generation models\n",
    "\n",
    "The study proposes two detector models:\n",
    "\n",
    "1. **Image-only detector**<br>binary classifier that decides whether an input image is fake or real.\n",
    "\n",
    "2. **Hybrid detector**<br>binary classifier that is able to tell if an image is fake or real, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "All the datasets are constitueted by a set of N real images (labeled 1), and a set of N corresponding fake generated images (labeled 0).\n",
    "\n",
    "Training (on a single dataset):\n",
    "- real images fetched from MSCOCO (class 1)\n",
    "- fake images generated by Stable Diffusion (SD) (class 0)\n",
    "\n",
    "Evaluation (on three different datasets):\n",
    "- real images always fetched from MSCOCO (class 1)\n",
    "- fake images generated respectively by Stable Diffusion (SD), Latent Diffusion (LD) and GLIDE (class 0)\n",
    "\n",
    "The data is structured as follows:\n",
    "\n",
    "imageonly_detector_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_0/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *fake images generated by LD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_1/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *real images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;├── val_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the path to the scripts needed for this section\n",
    "sys.path.insert(10, '/home/parwal/Documents/GitHub/De-Fake_nn_final_project/src/imageonly_detector')\n",
    "#TODO capire a chi serve questo import e metterlo nel posto giusto\n",
    "\n",
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO\n",
    "# The dataset generated using SD will be divided into train and test later on\n",
    "fetchImagesFromMSCOCO(\"data/MSCOCO_for_SD/images\", \"data/MSCOCO_for_SD\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# real images and captions needed as input for the LD model\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_LD/class_1\", \"data/imageonly_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "# real images and captions needed as input for the GLIDE model\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_detector_data/val_GLIDE/class_1\", \"data/imageonly_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "#prima di eseguire il file ho cambiato le directory\n",
    "#%run src/imageonly_detector/SD_MSCOCO_data_generation.py\n",
    "SD_generation(\"data/MSCOCO_for_SD/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "#resetto la directory corrente a quella del progetto de-fake, altrimenti il file da eseguire non viene trovato\n",
    "#questo è necessario perché LD_MSCOCO_data_generation.py cambia la directory a quella di latent-diffusion\n",
    "#os.chdir(\"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\")\n",
    "#%run src/imageonly_detector/LD_MSCOCO_data_generation.py\n",
    "LD_generation(\"data/imageonly_detector_data/val_LD/mscoco_captions.csv\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#%run src/imageonly_detector/GLIDE_MSCOCO_data_generation.ipynb\n",
    "GLIDE_generation(\"data/imageonly_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the collected data in the previously described structure\n",
    "\n",
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/MSCOCO_for_SD/images\", SD_generated_temp_dir, \"data/imageonly_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/imageonly_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/imageonly_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model\n",
    "\n",
    "The model is defined and trained in the file executed in the followind code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function trains the model and tests it at every epoch\n",
    "#both the test and train datasets are generated using SD\n",
    "%run src/imageonly_detector/train.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "\n",
    "The dataset is built in the exact same way as the dataset for the image-only detector.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- COLLECT REAL IMAGES FROM MSCOCO -------------------- #\n",
    "\n",
    "# TODO TEST\n",
    "\n",
    "#SD+MSCOCO\n",
    "fetchImagesFromMSCOCO(\"data/MSCOCO_for_SD_hybrid/images\", \"data/MSCOCO_for_SD_hybrid\", 100)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_LD/class_1\", \"data/hybrid_detector_data/val_LD\", 50)\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_detector_data/val_GLIDE/class_1\", \"data/hybrid_detector_data/val_GLIDE\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- GENERATE FAKE IMAGES USING SD, LD, GLIDE -------------------- #\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#use stable-diffusion API to generate 100 fake images from the 100 captions collected before\n",
    "#%run src/imageonly_detector/SD_MSCOCO_data_generation.py\n",
    "SD_generation(\"data/MSCOCO_for_SD_hybrid/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "# N.B.\n",
    "# prima di lanciare questo comando, aggiungere il file src/imageonly_detector/txt2img_batch.py alla directory latent-diffusion/scripts/\n",
    "#resetto la directory corrente a quella del progetto de-fake, altrimenti il file da eseguire non viene trovato\n",
    "#questo è necessario perché LD_MSCOCO_data_generation.py cambia la directory a quella di latent-diffusion\n",
    "#os.chdir(\"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\")\n",
    "#%run src/imageonly_detector/LD_MSCOCO_data_generation_batch.py\n",
    "LD_generation(\"data/hybrid_detector_data/val_LD/mscoco_captions.csv\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "#NON HO MAI PROVATO A RUNNARLO, altrimenti rigenera il modello (3gb)\n",
    "#provare a runnarlo proprio alla fine di tutto per sicurezza\n",
    "#%run src/imageonly_detector/GLIDE_MSCOCO_data_generation.ipynb #TODO\n",
    "GLIDE_generation(\"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\", GLIDE_generated_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FORMAT THE DATA INTO THE STRUCTURE NEEDED FOR TRAINING/TESTING -------------------- #\n",
    "os.chdir(\"/home/parwal/Documents/GitHub/De-Fake_nn_final_project\")\n",
    "\n",
    "#transform the collected data in the previously described structure\n",
    "\n",
    "#SD+MSCOCO --------------------------------------------------------------------------\n",
    "#this function generates a pair of datasets (train and val), starting from data from the Stable Diffusion generation\n",
    "#the data generated from SD contains 100 images, this original dataset is split in half (50 for train, 50 for test)\n",
    "formatIntoTrainTest(\"data/MSCOCO_for_SD_hybrid/images\", SD_generated_temp_dir, \"data/hybrid_detector_data\")\n",
    "print(\"ok SD\")\n",
    "\n",
    "#LD+MSCOCO --------------------------------------------------------------------------\n",
    "format_dataset_binaryclass(LD_generated_temp_dir, \"data/hybrid_detector_data/val_LD\")\n",
    "print(\"ok LD\")\n",
    "\n",
    "#GLIDE+MSCOCO -----------------------------------------------------------------------\n",
    "format_dataset_binaryclass(GLIDE_generated_temp_dir, \"data/hybrid_detector_data/val_GLIDE\")\n",
    "print(\"ok GLIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build and train a two-layer perceptron that acts as binary classifier.<br>\n",
    "Each input sample of this model is the encoding of an image concatenated with the encoding of its caption.<br><br>\n",
    "We build the create in three steps:\n",
    "- Build the model starting from a ppre-trained version of ResNet18\n",
    "- Build the Dataset and DataLoader objects for the raw data that we fetched before\n",
    "- Train the model with a custom train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "EPOCH:  1/10  - MEAN ACCURACY:  tensor(0.5000)  - MEAN LOSS:  tensor(0.6976)\n",
      "EPOCH:  2/10  - MEAN ACCURACY:  tensor(0.5400)  - MEAN LOSS:  tensor(0.6853)\n",
      "EPOCH:  3/10  - MEAN ACCURACY:  tensor(0.5900)  - MEAN LOSS:  tensor(0.6740)\n",
      "EPOCH:  4/10  - MEAN ACCURACY:  tensor(0.6200)  - MEAN LOSS:  tensor(0.6629)\n",
      "EPOCH:  5/10  - MEAN ACCURACY:  tensor(0.7800)  - MEAN LOSS:  tensor(0.6523)\n",
      "EPOCH:  6/10  - MEAN ACCURACY:  tensor(0.8200)  - MEAN LOSS:  tensor(0.6393)\n",
      "EPOCH:  7/10  - MEAN ACCURACY:  tensor(0.8700)  - MEAN LOSS:  tensor(0.6267)\n",
      "EPOCH:  8/10  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.6138)\n",
      "EPOCH:  9/10  - MEAN ACCURACY:  tensor(0.8900)  - MEAN LOSS:  tensor(0.5987)\n",
      "EPOCH:  10/10  - MEAN ACCURACY:  tensor(0.9300)  - MEAN LOSS:  tensor(0.5838)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Build the model\n",
    "print('Building the model...')\n",
    "hybrid_detector = TwoLayerPerceptron(1024, 100, 1).to(device)\n",
    "\n",
    "#Build the dataset\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_detector_data/mscoco_captions.csv\"\n",
    "real_img_dir = \"data/hybrid_detector_data/train/class_1\"\n",
    "fake_img_dir = \"data/hybrid_detector_data/train/class_0\"\n",
    "train_data_loader = get_dataset_loader(captions_file, real_img_dir, fake_img_dir)\n",
    "\n",
    "#Train the model\n",
    "print('Training starts')\n",
    "train_hybrid_detector(hybrid_detector, train_data_loader, 10, 0.005) # few epochs are needed because we use transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it on some test datasets.<br>\n",
    "In particular we will evaluate it on:<br>\n",
    "- Stable Diffusion (SD), dataset generated from the same image-to-text generator used for the train dataset.\n",
    "- GLIDE\n",
    "- Latent Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation starts:\n",
      "loading model with trained weights...\n",
      "testing...\n",
      "Evaluation on SD --> Accuracy: 0.8933333158493042 - Loss: 0.5959395170211792\n",
      "Evaluation on GLIDE --> Accuracy: 0.7100000381469727 - Loss: 0.6429540514945984\n",
      "Evaluation on LD --> Accuracy: 0.7100000977516174 - Loss: 0.6497437357902527\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the weights we trained in the previous code block\n",
    "print(\"loading model with trained weights...\")\n",
    "test_hybrid_detector = TwoLayerPerceptron(1024, 100, 1)\n",
    "test_hybrid_detector.load_state_dict(torch.load('trained_models/hybrid_detector.pth'))\n",
    "\n",
    "eval_dirs = {'SD': {\n",
    "                'captions': \"data/hybrid_detector_data/mscoco_captions.csv\", \n",
    "                'real': \"data/hybrid_detector_data/val/class_1\", \n",
    "                'fake': \"data/hybrid_detector_data/val/class_0\"},\n",
    "             'GLIDE': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_GLIDE/mscoco_captions.csv\",\n",
    "                  'real': \"data/hybrid_detector_data/val_GLIDE/class_1\", \n",
    "                  'fake': \"data/hybrid_detector_data/val_GLIDE/class_0\"},\n",
    "             'LD': {\n",
    "                 'captions': \"data/hybrid_detector_data/val_LD/mscoco_captions.csv\", \n",
    "                 'real': \"data/hybrid_detector_data/val_LD/class_1\", \n",
    "                 'fake': \"data/hybrid_detector_data/val_LD/class_0\"}}\n",
    "\n",
    "#Build a the dataloaders and test the model on each of them\n",
    "print(\"Evaluation starts\")\n",
    "for dataset_name in eval_dirs:\n",
    "    eval_data_loader = get_dataset_loader(eval_dirs[dataset_name]['captions'], eval_dirs[dataset_name]['real'], eval_dirs[dataset_name]['fake'])\n",
    "    SDloss, SDacc = eval_hybrid_detector(test_hybrid_detector, eval_data_loader)\n",
    "    print(f'Evaluation on {dataset_name} --> Accuracy: {SDacc} - Loss: {SDloss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2. Attribution of the fake images to their source model\n",
    "\n",
    "The study proposes two attributor models:\n",
    "\n",
    "1. **Image-only attributor**<br>multi-class classifier that assigns each input image to its source generation model, given the image only.\n",
    "\n",
    "2. **Hybrid attributor**<br>multi-class classifier that assigns each input image to its source generation model, based on the input image and its corresponding text prompt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Image-only attributor\n",
    "\n",
    "In this section we will build and train a model that is able to assign an image to the model that generated it, given only that image.<br><br>\n",
    "The classes that this model will be able to address are the following:\n",
    "- real image -> class 0\n",
    "- fake image generated by SD -> class 1\n",
    "- fake image generated by LD -> class 2\n",
    "- fake image generated by GLIDE -> class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset\n",
    "\n",
    "We generate two datasets, one for training and one for evaluating the model.<br>\n",
    "The steps needed to generate the two datasets are the same:\n",
    "- fetch real images and their captions from MSCOCO (class 0)\n",
    "- generate fake images with SD using the captions of the real images (class 1)\n",
    "- generate fake images with LD using the captions of the real images (class 2)\n",
    "- generate fake images with GLIDE using the captions of the real images (class 3)\n",
    "- move the real and generated images into a dataset directory, with the following structure:\n",
    "\n",
    "imageonly_attributor_data/<br>\n",
    "&emsp;&emsp;├── train/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_real/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images fetched by MSCOCO*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_SD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by SD*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_GLIDE/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by GLIDE*<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── class_LD/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   ├── ...<br>\n",
    "&emsp;&emsp;&emsp;&emsp;│   └── *all the images generated by LD*<br>\n",
    "&emsp;&emsp;├── test/<br>\n",
    "&emsp;&emsp;&emsp;&emsp;├── ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(proj_dir)\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/train/class_real\", \"data/imageonly_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/train/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same procedure for the test dataset\n",
    "\n",
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/imageonly_attributor_data/test/class_real\", \"data/imageonly_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD\n",
    "LD_generation(\"data/imageonly_attributor_data/test/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/imageonly_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 5.9964 Acc: 0.1800\n",
      "test Loss: 6.0245 Acc: 0.2601\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.2186 Acc: 0.6100\n",
      "test Loss: 3.0599 Acc: 0.5480\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.6677 Acc: 0.6950\n",
      "test Loss: 3.4734 Acc: 0.5294\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.1393 Acc: 0.7450\n",
      "test Loss: 1.9173 Acc: 0.5851\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.6820 Acc: 0.8300\n",
      "test Loss: 1.0026 Acc: 0.7461\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.6870 Acc: 0.8250\n",
      "test Loss: 1.2763 Acc: 0.7028\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.6716 Acc: 0.8300\n",
      "test Loss: 1.4615 Acc: 0.6749\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.9099 Acc: 0.7600\n",
      "test Loss: 1.5547 Acc: 0.6780\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.6681 Acc: 0.7800\n",
      "test Loss: 1.2790 Acc: 0.6904\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.5464 Acc: 0.8450\n",
      "test Loss: 1.3354 Acc: 0.6842\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.5563 Acc: 0.8600\n",
      "test Loss: 1.3004 Acc: 0.6130\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.4239 Acc: 0.8500\n",
      "test Loss: 1.0085 Acc: 0.6997\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.4823 Acc: 0.8350\n",
      "test Loss: 1.3160 Acc: 0.6099\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.5948 Acc: 0.8100\n",
      "test Loss: 1.2332 Acc: 0.6656\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.4194 Acc: 0.8550\n",
      "test Loss: 1.2181 Acc: 0.5944\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.4779 Acc: 0.8400\n",
      "test Loss: 1.0136 Acc: 0.6811\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.6291 Acc: 0.8200\n",
      "test Loss: 1.4138 Acc: 0.6068\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.4776 Acc: 0.8250\n",
      "test Loss: 1.5359 Acc: 0.6687\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.3972 Acc: 0.8750\n",
      "test Loss: 1.3901 Acc: 0.6502\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.3408 Acc: 0.8550\n",
      "test Loss: 1.2251 Acc: 0.6440\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.4252 Acc: 0.8850\n",
      "test Loss: 1.4503 Acc: 0.6718\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.3788 Acc: 0.8650\n",
      "test Loss: 1.0702 Acc: 0.7059\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4188 Acc: 0.8550\n",
      "test Loss: 1.1864 Acc: 0.6533\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.3185 Acc: 0.8950\n",
      "test Loss: 0.8095 Acc: 0.7245\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.2655 Acc: 0.9050\n",
      "test Loss: 0.9268 Acc: 0.6997\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.4487 Acc: 0.8500\n",
      "test Loss: 1.2250 Acc: 0.6749\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.2869 Acc: 0.9050\n",
      "test Loss: 0.9883 Acc: 0.6780\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.3221 Acc: 0.9050\n",
      "test Loss: 0.9379 Acc: 0.7214\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.4443 Acc: 0.8650\n",
      "test Loss: 0.9975 Acc: 0.7028\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.3433 Acc: 0.8800\n",
      "test Loss: 1.1611 Acc: 0.6718\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.3186 Acc: 0.9100\n",
      "test Loss: 1.1458 Acc: 0.6594\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3882 Acc: 0.8750\n",
      "test Loss: 1.6915 Acc: 0.6130\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.3654 Acc: 0.8550\n",
      "test Loss: 1.1687 Acc: 0.6718\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.3958 Acc: 0.8600\n",
      "test Loss: 1.5062 Acc: 0.6502\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.3035 Acc: 0.9150\n",
      "test Loss: 2.4686 Acc: 0.5232\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.3042 Acc: 0.8850\n",
      "test Loss: 1.3312 Acc: 0.6780\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.2005 Acc: 0.9200\n",
      "test Loss: 1.2574 Acc: 0.6656\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.3741 Acc: 0.8750\n",
      "test Loss: 1.4179 Acc: 0.6625\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.2937 Acc: 0.8800\n",
      "test Loss: 1.0822 Acc: 0.6533\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2048 Acc: 0.9350\n",
      "test Loss: 1.4240 Acc: 0.6533\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.3780 Acc: 0.8850\n",
      "test Loss: 1.1834 Acc: 0.7028\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.3184 Acc: 0.8850\n",
      "test Loss: 1.4166 Acc: 0.6904\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.3699 Acc: 0.8950\n",
      "test Loss: 2.0267 Acc: 0.6223\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.2340 Acc: 0.9400\n",
      "test Loss: 1.2433 Acc: 0.7214\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.3277 Acc: 0.8750\n",
      "test Loss: 1.1359 Acc: 0.7121\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.2888 Acc: 0.8850\n",
      "test Loss: 1.2981 Acc: 0.6842\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.1572 Acc: 0.9400\n",
      "test Loss: 1.0580 Acc: 0.7307\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.3372 Acc: 0.9100\n",
      "test Loss: 0.9491 Acc: 0.7183\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.3475 Acc: 0.8900\n",
      "test Loss: 0.9771 Acc: 0.6842\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.1938 Acc: 0.9400\n",
      "test Loss: 1.0887 Acc: 0.6749\n",
      "Best val Acc: 0.746130\n",
      "Evaluation starts:\n",
      "TEST LOSS: 1.0026 ACC: 0.7461\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Build the datasets\n",
    "print(\"Building the dataset...\")\n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomResizedCrop(224),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'data/imageonly_attributor_data'\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training starts:\")\n",
    "trained_model = train_imageonly_attributor(model, dataloaders, dataset_sizes, num_epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluation starts:\")\n",
    "eval_imageonly_attributor(trained_model, dataloaders, dataset_sizes) #valuta se serve, in fondo il test lo fai durante il training... viene uguale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hybrid attributor\n",
    "In this section we will build and train a model similar to the model built in section 1.<br>\n",
    "The difference is that instead of taking as input only the image, this model also considers its textual caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test datasets are generated in the same way as in the image-only attributor case.<br>\n",
    "For the dataset directory structure also refer to the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the TRAIN dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/train/class_real\", \"data/hybrid_attributor_data/train\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/train/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then denerate the TEST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the images with their captions from MSCOCO (N=50)\n",
    "fetchImagesFromMSCOCO(\"data/hybrid_attributor_data/test/class_real\", \"data/hybrid_attributor_data/test\", 50)\n",
    "\n",
    "# use the same 50 captions to generate images with SD\n",
    "SD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", SD_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with GLIDE\n",
    "GLIDE_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\", GLIDE_generated_temp_dir)\n",
    "\n",
    "# use the same 50 captions to generate images with LD OK\n",
    "LD_generation(\"data/hybrid_attributor_data/test/mscoco_captions.csv\")\n",
    "\n",
    "# move the generated images to the dataset dir\n",
    "format_dataset_multiclass(SD_generated_temp_dir, LD_generated_temp_dir, GLIDE_generated_temp_dir, \"data/hybrid_attributor_data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block we build and train the actual model, a two-layer perceptron for multiclass classification, with the following steps:\n",
    "- **Build the model** using a custom implemented module<br>A two-layer perceptron that outputs the class predicted for each sample<br><br>\n",
    "- **Create Dataset and DataLoader** objects starting from the row data fetched at 2.1 (jpg images and string captions)<br>Each item of this dataset is composed by the encoding of an image concatenated with the encoding of its caption,<br>the encodings are generated using the CLIP model<br><br>\n",
    "- **Train the model** using a custom train function and the DataLoader from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Building the dataset...\n",
      "Training starts:\n",
      "epoch: 1/30\n",
      "EPOCH:  1  - MEAN ACCURACY:  tensor(0.2200)  - MEAN LOSS:  tensor(1.3872)\n",
      "epoch: 2/30\n",
      "EPOCH:  2  - MEAN ACCURACY:  tensor(0.3183)  - MEAN LOSS:  tensor(1.3617)\n",
      "epoch: 3/30\n",
      "EPOCH:  3  - MEAN ACCURACY:  tensor(0.4183)  - MEAN LOSS:  tensor(1.3365)\n",
      "epoch: 4/30\n",
      "EPOCH:  4  - MEAN ACCURACY:  tensor(0.5933)  - MEAN LOSS:  tensor(1.3108)\n",
      "epoch: 5/30\n",
      "EPOCH:  5  - MEAN ACCURACY:  tensor(0.6767)  - MEAN LOSS:  tensor(1.2796)\n",
      "epoch: 6/30\n",
      "EPOCH:  6  - MEAN ACCURACY:  tensor(0.7567)  - MEAN LOSS:  tensor(1.2472)\n",
      "epoch: 7/30\n",
      "EPOCH:  7  - MEAN ACCURACY:  tensor(0.7767)  - MEAN LOSS:  tensor(1.2103)\n",
      "epoch: 8/30\n",
      "EPOCH:  8  - MEAN ACCURACY:  tensor(0.7817)  - MEAN LOSS:  tensor(1.1686)\n",
      "epoch: 9/30\n",
      "EPOCH:  9  - MEAN ACCURACY:  tensor(0.8517)  - MEAN LOSS:  tensor(1.1257)\n",
      "epoch: 10/30\n",
      "EPOCH:  10  - MEAN ACCURACY:  tensor(0.7917)  - MEAN LOSS:  tensor(1.0752)\n",
      "epoch: 11/30\n",
      "EPOCH:  11  - MEAN ACCURACY:  tensor(0.8250)  - MEAN LOSS:  tensor(1.0234)\n",
      "epoch: 12/30\n",
      "EPOCH:  12  - MEAN ACCURACY:  tensor(0.8917)  - MEAN LOSS:  tensor(0.9716)\n",
      "epoch: 13/30\n",
      "EPOCH:  13  - MEAN ACCURACY:  tensor(0.8350)  - MEAN LOSS:  tensor(0.9214)\n",
      "epoch: 14/30\n",
      "EPOCH:  14  - MEAN ACCURACY:  tensor(0.8517)  - MEAN LOSS:  tensor(0.8695)\n",
      "epoch: 15/30\n",
      "EPOCH:  15  - MEAN ACCURACY:  tensor(0.9000)  - MEAN LOSS:  tensor(0.8177)\n",
      "epoch: 16/30\n",
      "EPOCH:  16  - MEAN ACCURACY:  tensor(0.8950)  - MEAN LOSS:  tensor(0.7632)\n",
      "epoch: 17/30\n",
      "EPOCH:  17  - MEAN ACCURACY:  tensor(0.9150)  - MEAN LOSS:  tensor(0.7183)\n",
      "epoch: 18/30\n",
      "EPOCH:  18  - MEAN ACCURACY:  tensor(0.9100)  - MEAN LOSS:  tensor(0.6733)\n",
      "epoch: 19/30\n",
      "EPOCH:  19  - MEAN ACCURACY:  tensor(0.9300)  - MEAN LOSS:  tensor(0.6240)\n",
      "epoch: 20/30\n",
      "EPOCH:  20  - MEAN ACCURACY:  tensor(0.9067)  - MEAN LOSS:  tensor(0.5921)\n",
      "epoch: 21/30\n",
      "EPOCH:  21  - MEAN ACCURACY:  tensor(0.9400)  - MEAN LOSS:  tensor(0.5433)\n",
      "epoch: 22/30\n",
      "EPOCH:  22  - MEAN ACCURACY:  tensor(0.9400)  - MEAN LOSS:  tensor(0.5106)\n",
      "epoch: 23/30\n",
      "EPOCH:  23  - MEAN ACCURACY:  tensor(0.9450)  - MEAN LOSS:  tensor(0.4734)\n",
      "epoch: 24/30\n",
      "EPOCH:  24  - MEAN ACCURACY:  tensor(0.9500)  - MEAN LOSS:  tensor(0.4463)\n",
      "epoch: 25/30\n",
      "EPOCH:  25  - MEAN ACCURACY:  tensor(0.9600)  - MEAN LOSS:  tensor(0.4143)\n",
      "epoch: 26/30\n",
      "EPOCH:  26  - MEAN ACCURACY:  tensor(0.9600)  - MEAN LOSS:  tensor(0.3839)\n",
      "epoch: 27/30\n",
      "EPOCH:  27  - MEAN ACCURACY:  tensor(0.9667)  - MEAN LOSS:  tensor(0.3675)\n",
      "epoch: 28/30\n",
      "EPOCH:  28  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3377)\n",
      "epoch: 29/30\n",
      "EPOCH:  29  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.3170)\n",
      "epoch: 30/30\n",
      "EPOCH:  30  - MEAN ACCURACY:  tensor(0.9800)  - MEAN LOSS:  tensor(0.2977)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build the model\n",
    "print('Building the model...')\n",
    "hybrid_attributor = MultiClassTwoLayerPerceptron(1024, 100, 4).to(device)\n",
    "\n",
    "# Build the dataset (each sample in the dataset is the encoding of an image concatenated to the encoding of its caption - encodings generated using the CLIP model)\n",
    "print('Building the dataset...')\n",
    "captions_file = \"data/hybrid_attributor_data/train/mscoco_captions.csv\"\n",
    "dataset_dir = \"data/hybrid_attributor_data/train\"\n",
    "classes = {\"class_real\", \"class_SD\", \"class_LD\", \"class_GLIDE\"}\n",
    "\n",
    "train_data_loader = get_multiclass_dataset_loader(captions_file, dataset_dir, classes)\n",
    "\n",
    "# Train the model on the dataset just generated\n",
    "print('Training starts:')\n",
    "train_hybrid_attributor(hybrid_attributor, train_data_loader, 30, 0.005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3. Analysis of the likelihood that different text prompts have to generate authentic images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
